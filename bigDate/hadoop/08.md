[TOC]

# 08_sqoop
## 1. 搭建sqoop
### 1.1 上传文件
### 1.2 修改配置文件
1. 修改名称     
 `/conf/sqoop-env-template.sh`名字为`/conf/sqoop-env.sh` 
2. 修改文件内容     
```
export HADOOP_COMMON_HOME=
export HADOOP_MAPRED_HOME=
export HIVE_HOME=
```
在vim下 直接获取地址 :r!whice hadoop

3. 上传jdbc jar包
上传到 /lib下

4. 配进环境
```
vi /etc/profile

export SQOOP_HOME=/root/apps/sqoop/sqoop
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin:$SQOOP_HOME/bin

source /etc/profile
```

## 2 测试启动
```
./sqoop-list-databases --connect jdbc:mysql://mycp:3306/ --username root --password root
```
```
./sqoop-list-tables --connect jdbc:mysql://mycp:3306/bigdata --username root --password root
```

## 3 导入导出数据

### 3.1 修改每台服务器的mapred-site.xml.template

mv mapred-site.xml.template mapred-site.xml
```
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>
```

### 3.2 导入表表数据到HDFS
```
./sqoop import \
--connect jdbc:mysql://mycp:3306/bigdata \
--username root \
--password root \
--target-dir \
/sqooptest \
--fields-terminated-by ',' \
--table emp \
--split-by id \
--m 2
```
如果出现错误
```
执行mapreduce没报错，可是任务运行到running job就卡住在

INFO mapreduce.Job: Running job: job14039055428930004

解决方法 mapred-site.xml下将

<property>
       <name>mapreduce.framework.name</name>
       <value>yarn</value>
</property>
改成

<property>
      <name>mapreduce.job.tracker</name>
      <value>hdfs://192.168.1.120:8001</value>
      <final>true</final>
 </property>
其中ip是master地址。
```
### 3.3 导入表表数据到Hive

#### 3.3.1 导入全部的数据
```
./sqoop import \
--hive-import \
--connect jdbc:mysql://mycp:3306/bigdata \
--username root \
--password root \
--table emp \
--split-by id \
--m 2
```
```
hive导入参数
　　--hive-home   重写$HIVE_HOME
　　--hive-import          插入数据到hive当中，使用hive的默认分隔符
　　--hive-overwrite  重写插入
　　--create-hive-table  建表，如果表已经存在，该操作会报错！
　　--hive-table [table]  设置到hive当中的表名
　　--hive-drop-import-delims  导入到hive时删除 \n, \r, and \01 
　　--hive-delims-replacement  导入到hive时用自定义的字符替换掉 \n, \r, and \01 
　　--hive-partition-key          hive分区的key
　　--hive-partition-value   hive分区的值
　　--map-column-hive           类型匹配，sql类型对应到hive类型
```

如果出现错误
```
18/12/12 11:25:35 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf
```

```
复制hive lib 包下面的hive-common-1.1.0-cdh5.7.0.jar 至sqoop lib 包下
```

#### 3.3.2 导入部分数据
##### 3.3.2.1 自定义导入部分数据
- 方法1（导入dfs）：
```
sqoop import \
--connect jdbc:mysql://mycp:3306/bigdata \
--username root \
--password root \
--where "city ='sec-bad'" \
--target-dir /wherequery \
--table emp_add \
 --m 1
```
- 方法2（导入dfs）：
```
sqoop import \
--connect jdbc:mysql://mycp:3306/bigdata \
--username root \
--password root \
--target-dir /wherequery2 \
--query 'select id,name,deg from emp WHERE id>1207 and $CONDITIONS' \
--split-by id \
--fields-terminated-by '\t' \
--m 2

```
`and $CONDITIONS`为固定写法

##### 3.3.2.2 增量导入
sqoop支持两种增量MySql导入到hive的模式一种是append，
1. 即通过指定一个递增的列  
`--incremental append  --check-column num_id --last-value 0 `   


2. 是可以根据时间戳，比如：     
`--incremental lastmodified --check-column created --last-value '2012-02-01 11:0:00' `      
就是只导入created 比'2012-02-01 11:0:00'更大的数据。

导入dfs——demo:
```
sqoop import \
--connect jdbc:mysql://mycp:3306/bigdata \
--target-dir \
/sqooptest \
--username root \
--password root \
--table emp --m 1 \
--incremental append \
--check-column id \
--last-value 1205
```
`--incremental append \`追加

==注意：增量导入不适合直接导入hive 只能通过导入hdfs导入hive       
分隔符最后 自定义 否则追加到hive 很容易出问题 (默认分隔符为\001)==

### 3.4 导出数据到mysql

```
sqoop export \
--connect jdbc:mysql://mycp:3306/bigdata \
--username root \
--password root \
--fields-terminated-by '\001' \
--table emp_add \
--export-dir /user/hive/warehouse/emp_add

```