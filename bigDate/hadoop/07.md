[TOC]

# 07_hive_搭建_启动_使用
## 1. 搭建hive 
### 1.1 上传hive安装包
1. 上传apache-hive-1.2.1-bin.tar.gz 压缩包     
2. 解压
3. 在conf下面新建一个   hive-site.xml  文件     

hive-site.xml
```xml
<configuration>
<property>
<name>javax.jdo.option.ConnectionURL</name>
<value>jdbc:mysql://192.169.1.221:3306/hive?createDatabaseIfNotExist=true</value>
<description>JDBC connect string for a JDBC metastore</description>
</property>

<property>
<name>javax.jdo.option.ConnectionDriverName</name>
<value>com.mysql.jdbc.Driver</value>
<description>Driver class name for a JDBC metastore</description>
</property>

<property>
<name>javax.jdo.option.ConnectionUserName</name>
<value>root</value>
<description>username to use against metastore database</description>
</property>

<property>
<name>javax.jdo.option.ConnectionPassword</name>
<value>root</value>
<description>password to use against metastore database</description>
</property>
</configuration>
```

4. 上传mysql驱动jar包 到lib下
5. 在bin 中  hive即可启动


### 1.2 hive基本思想
Hive是基于Hadoop的一个数据仓库工具(离线)，可以将结构化的数据文件映射为一张数据库表，并提供类SQL查询功能。

### 1.3 为什么使用Hive
1.  直接使用hadoop所面临的问题  
人员学习成本太高    
项目周期要求太短    
MapReduce实现复杂查询逻辑开发难度太大 
2.	为什么要使用Hive    
操作接口采用类SQL语法，提供快速开发的能力。     
避免了去写MapReduce，减少开发人员的学习成本。   
功能扩展很方便。    

### 1.4 Hive特点
1.  可扩展  
Hive可以自由的扩展集群的规模，一般情况下不需要重启服务。
2.  延展性  
Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。
3. 容错     
良好的容错性，节点出现问题SQL仍可完成执行。

## 2 基本使用

### 2.1 优化使用
1. 让提示符显示当前库： 

```
hive> set hive.cli.print.current.db=true;
```

2、显示查询结果时显示字段名称：

```
hive>set hive.cli.print.header=true;
```


但是这样设置只对当前会话有效，重启hive会话后就失效      
解决办法：
在linux的当前用户目录中，编辑一个.hiverc文件    
将参数写入其中：    
```
vi .hiverc  
set hive.cli.print.header=true;
set hive.cli.print.current.db=true;
```

### 2.2 将hive 配置进系统变量
```
vi /etc/profile

export HIVE_HOME=/root/apps/hive/apache-hive-1.2.1-bin
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin

source /etc/profile
```

### 2.3 启动hive服务使用
启动hive的服务：

```
bin/hiveserver2 -hiveconf hive.root.logger=DEBUG,console
```
上述启动，会将这个服务启动在前台，如果要启动在后台，则命令如下：

```
nohup bin/hiveserver2 1>/dev/null 2>&1 &
```
启动成功后，可以在别的节点上用beeline去连接
- 方式（1）     
```
bin/beeline
```
回车，进入beeline的命令界面
输入命令连接hiveserver2

```
beeline> !connect jdbc:hive2//hadoop01:10000
（hadoop01是hiveserver2所启动的那台主机名，端口默认是10000）
```

- 方式（2）
启动时直接连接：

```
bin/beeline -u jdbc:hive2://vm03:10000 -n root
```


接下来就可以做正常sql查询了


### 2.4	脚本化运行
大量的hive查询任务，如果用交互式shell来进行输入的话，显然效率及其低下，因此，生产中更多的是使用脚本化运行机制：  
该机制的核心点是：hive可以用一次性命令的方式来执行给定的hql语句

```
hive -e "insert into table t_dest select * from t_src;"
```

然后，进一步，可以将上述命令写入shell脚本中，以便于脚本化运行hive任务，并控制、调度众多hive任务，示例如下：     

```
vi x.sh
```

```
#!/bin/bash
hive -e "select * from db_order.t_order"
hive -e "select * from default.t_user"
hql="create table  default.t_bash as select * from db_order.t_order"
hive -e "$hql"
```

如果要执行的hql语句特别复杂，那么，可以把hql语句写入一个文件：  

```
vi x.hql
```

```
select * from db_order.t_order;
select count(1) from db_order.t_user;
```


然后，用
```
hive -f /root/x.hql
```
 来执行


## 3	hive建库建表与数据导入
### 3.1 库
#### 3.1.1 建库 
hive中有一个默认的库：  
库名： default  
库目录：hdfs://hdp20-01:9000/user/hive/warehouse    

新建库：    
```
create database db_order;
```
库建好后，在hdfs中会生成一个库目录：    
user/hive/warehouse/db_order.db

### 3.2 表
#### 3.2.1 基本建表语句

```
use db_order;//进入db_order库

create table t_order(id string,create_time string,amount float,uid string);
```

表建好后，会在所属的库目录中生成一个表目录

```
/user/hive/warehouse/db_order.db/t_order
```

只是，这样建表的话，hive会认为表数据文件中的字段分隔符为 ^A（vim 中输入^A 为Alt+a）

正确的建表语句为：

```
create table t_order(id string,create_time string,amount float,uid string)
row format delimited
fields terminated by ',';
```

这样就指定了表数据文件中的字段分隔符为 ","

#### 3.2.2	删除表

```
drop table t_order;
```

删除表的效果是：    
- hive会从元数据库中清除关于这个表的信息；    
- hive还会从hdfs中删除这个表的表目录；    

#### 3.2.3	修改表定义

仅修改Hive元数据，不会触动表中的数据，用户需要确定实际的数据布局符合元数据的定义。

##### 3.2.3.1 修改表名：
==alter table== name  ==rename to== new_name


##### 3.2.3.2 修改分区名：

```
alter table t_partition partition(department='xiangsheng',sex='male',howold=20) rename to partition(department='1',sex='1',howold=20);
```

##### 3.2.3.1 添加分区：

```
alter table t_partition add partition (department='2',sex='0',howold=40);
```


##### 3.2.3.1 删除分区：

```
alter table t_partition drop partition (department='2',sex='2',howold=24);
```


##### 3.2.3.1 修改表的文件格式定义：

```
ALTER TABLE table_name [PARTITION partitionSpec] SET FILEFORMAT file_format

alter table t_partition partition(department='2',sex='0',howold=40 ) set fileformat sequencefile;
```



##### 3.2.3.1 修改列名定义：

```
ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENTcol_comment] [FIRST|(AFTER column_name)]  
alter table t_user change price jiage float first;
```


##### 3.2.3.1 增加/替换列：

```
ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type[COMMENT col_comment], ...)  

alter table t_user add columns (sex string,addr string);
alter table t_user replace columns (id string,age int,price float);
```


### 3.3 内部表与外部表
内部表(MANAGED_TABLE)：     
表目录按照hive的规范来部署，位于hive的仓库目录/user/hive/warehouse中  

外部表(EXTERNAL_TABLE)：    
表目录由建表用户自己指定

```
create external table t_access(ip string,url string,access_time string)
row format delimited
fields terminated by ','
location '/access/log';
```

外部表和内部表的特性差别：  
1. 内部表的目录在hive的仓库目录中 VS 外部表的目录由用户指定 
2. drop一个内部表时：hive会清除相关元数据，并删除表数据目录 
3. drop一个外部表时：hive只会清除相关元数据；   

一个hive的数据仓库，最底层的表，一定是来自于外部系统，为了不影响外部系统的工作逻辑，在hive中可建external表来映射这些外部系统产生的数据目录；    
然后，后续的etl操作，产生的各种表建议用managed_table

### 3.4 	分区表
分区表的实质是：在表目录中为数据文件创建分区子目录，以便于在查询时，MR程序可以针对分区子目录中的数据进行处理，缩减读取数据的范围。

比如，网站每天产生的浏览记录，浏览记录应该建一个表来存放，但是，有时候，我们可能只需要对某一天的浏览记录进行分析
这时，就可以将这个表建为分区表，每天的数据导入其中的一个分区；
当然，每日的分区目录，应该有一个目录名（分区字段）

---------
#### demo1：  

##### 1. 创建带分区的表
```
create table t_access(ip string,url string,access_time string)
partitioned by(dt string)
row format delimited
fields terminated by ',';
```
注意：分区字段不能是表定义中的已存在字段


##### 2. 向分区中导入数据
```
load data local inpath '/root/access.log.2017-08-04.log' into table t_access partition(dt='20170804');
load data local inpath '/root/access.log.2017-08-05.log' into table t_access partition(dt='20170805');
```

##### 3. 针对分区数据进行查询     

a. 统计8月4号的总PV：
```
select count(*) from t_access where dt='20170804';
```
实质：就是将分区字段当成表字段来用，就可以使用where子句指定分区了

b、统计表中所有数据总的PV：
```
select count(*) from t_access;
```
实质：不指定分区条件即可

---------

#### demo2:
多个分区字段示例

##### 建表：  

```
create table t_partition(id int,name string,age int)
partitioned by(department string,sex string,howold int)
row format delimited fields terminated by ',';
```

##### 导数据：

```
load data local inpath '/root/p1.dat' into table t_partition partition(department='xiangsheng',sex='male',howold=20);
```
---------

### 3.5	CTAS建表语法

1.可以通过已存在表来建表：  
```
create table t_user_2 like t_user;
//新建的t_user_2表结构定义与源表t_user一致，但是没有数据
```

2.在建表的同时插入数据

```
create table t_access_user 
as
select ip,url from t_access;
```
t_access_user会根据select查询的字段来建表，同时将查询的结果插入新表中

### 3.6	数据导入导出
#### 3.6.1 将数据文件导入hive的表
- 方式1：导入数据的一种方式：   
手动用hdfs命令，将文件放入表目录；

- 方式2：在hive的交互式shell中用hive命令来导入本地数据到表目录

```
hive>load data local inpath '/root/order.data.2' into table t_order;
```


- 方式3：用hive命令导入hdfs中的数据文件到表目录

```
hive>load data inpath '/access.log.2017-08-06.log' into table t_access partition(dt='20170806');
```

==注意：导本地文件和导HDFS文件的区别：
本地文件导入表：复制
hdfs文件导入表：移动==



#### 3.6.1 将hive表中的数据导出到指定路径的文件
1. 将hive表中的数据导入HDFS的文件

```
insert overwrite directory '/root/access-data'
row format delimited fields terminated by ','
select * from t_access;
```

2. 将hive表中的数据导入本地磁盘文件

```
insert overwrite local directory '/root/access-data'
row format delimited fields terminated by ','
select * from t_access limit 100000;
```



#### 3.6.2	hive文件格式
HIVE支持很多种文件格式：

```
SEQUENCE FILE | TEXT FILE | PARQUET FILE | RC FILE
create table t_pq(movie string,rate int)  stored as textfile;
create table t_pq(movie string,rate int)  stored as sequencefile;
create table t_pq(movie string,rate int)  stored as parquetfile;
```

demo：
1. 
先建一个存储文本文件的表

```
create table t_access_text(ip string,url string,access_time string)
row format delimited fields terminated by ','
stored as textfile;
```

导入文本数据到表中：

```
load data local inpath '/root/access-data/000000_0' into table t_access_text;
```



2.  
建一个存储sequence file文件的表：

```
create table t_access_seq(ip string,url string,access_time string)
stored as sequencefile;
```

从文本表中查询数据插入sequencefile表中，生成数据文件就是sequencefile格式的了：

```
insert into t_access_seq
select * from t_access_text;
```


3.
建一个存储parquet file文件的表：

```
create table t_access_parq(ip string,url string,access_time string)
stored as parquetfile;
```

### 3.7 数据类型
#### 3.7.1 数字类型

```
TINYINT (1-byte signed integer, from -128 to 127)
SMALLINT (2-byte signed integer, from -32,768 to 32,767)

INT/INTEGER (4-byte signed integer, from -2,147,483,648 to 2,147,483,647)

BIGINT (8-byte signed integer, from -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807)
FLOAT (4-byte single precision floating point number)
DOUBLE (8-byte double precision floating point number)
```

demo:
```
create table t_test(a string ,b int,c bigint,d float,e double,f tinyint,g smallint)
```


#### 3.7.2	日期时间类型

```
TIMESTAMP (Note: Only available starting with Hive 0.8.0)  时间戳
DATE (Note: Only available starting with Hive 0.12.0)
```
demo:
```
假如有以下数据文件：
1,zhangsan,1985-06-30
2,lisi,1986-07-10
3,wangwu,1985-08-09
那么，就可以建一个表来对数据进行映射

create table t_customer(id int,name string,birthday date)
row format delimited 
fields terminated by ',';

然后导入数据
load data local inpath '/root/customer.dat' into table t_customer;
然后，就可以正确查询
```


#### 3.7.3	字符串类型

```
STRING
VARCHAR (Note: Only available starting with Hive 0.12.0)
CHAR (Note: Only available starting with Hive 0.13.0)
```

#### 3.7.4	混杂类型

```
BOOLEAN
BINARY (Note: Only available starting with Hive 0.8.0)
```


#### 3.7.5	复合类型
##### 3.7.5.1	array数组类型

```
arrays: ARRAY<data_type> (Note: negative values and non-constant expressions are allowed as of Hive 0.14.)
```


```
示例：array类型的应用
假如有如下数据需要用hive的表去映射：    

流浪地球,屈楚萧:吴京:李光洁:吴孟达:赵今麦,2019-02-05
飞驰人生,沈腾:黄景瑜:尹正:张本煜:尹昉,2019-02-05
疯狂的外星人,黄渤:沈腾:汤姆·派福瑞:马修·莫里森:徐峥,2019-02-05  

设想：如果主演信息用一个数组来映射比较方便

建表：
create table t_movie(moive_name string,actors array<string>,first_show date)
row format delimited fields terminated by ','
collection items terminated by ':';

导入数据：
load data local inpath '/root/movie.dat' into table t_movie;

查询：
select * from t_movie;
select moive_name,actors[0] from t_movie;
select moive_name,actors from t_movie where array_contains(actors,'吴刚');//array_contains(actors,'吴刚');是否存在
select moive_name,size(actors) from t_movie;
```

##### 3.7.5.2	map类型
```
maps: MAP<primitive_type, data_type> (Note: negative values and non-constant expressions are allowed as of Hive 0.14.)
```
```
1)	假如有以下数据：
1,zhangsan,father:xiaoming#mother:xiaohuang#brother:xiaoxu,28
2,lisi,father:mayun#mother:huangyi#brother:guanyu,22
3,wangwu,father:wangjianlin#mother:ruhua#sister:jingtian,29
4,mayun,father:mayongzhen#mother:angelababy,26

可以用一个map类型来对上述数据中的家庭成员进行描述

2)	建表语句：
create table t_person(id int,name string,family_members map<string,string>,age int)
row format delimited fields terminated by ','
collection items terminated by '#'
map keys terminated by ':';

3)	查询
select * from t_person;

## 取map字段的指定key的值
select id,name,family_members['father'] as father from t_person;

## 取map字段的所有key
select id,name,map_keys(family_members) as relation from t_person;

## 取map字段的所有value
select id,name,map_values(family_members) from t_person;
select id,name,map_values(family_members)[0] from t_person;

## 综合：查询有brother的用户信息
select 
    id,name,father 
from (
    select 
        id,name,family_members['brother'] as father 
    from 
        t_person) tmp
where 
    father 
is not null;

或
select  
    id,name,age
from 
    t_family 
where
    array_contains(map_keys(family_members),'brother');
```



##### 3.7.5.3	struct类型

```
structs: STRUCT<col_name : data_type, ...>
```


```
1)	假如有如下数据：
1,zhangsan,18:male:beijing
2,lisi,28:female:shanghai

其中的用户信息包含：年龄：整数，性别：字符串，地址：字符串
设想用一个字段来描述整个用户信息，可以采用struct

2)	建表：
create table t_person_struct(id int,name string,info struct<age:int,sex:string,addr:string>)
row format delimited fields terminated by ','
collection items terminated by ':';

3)	查询
select * from t_person_struct;
select id,name,info.age from t_person_struct;
```






### 3.7	join关联查询示例
数据:

```
+------------+------------+--+
| t_a.name1  | t_a.name2  |
+------------+------------+--+
| a          | 1          |
| b          | 2          |
| c          | 3          |
| d          | 4          |
+------------+------------+--+
```
```
+------------+------------+--+
| t_b.name1  | t_b.name2  |
+------------+------------+--+
| a          | xx         |
| b          | yy         |
| d          | zz         |
| e          | pp         |
+------------+------------+--+
```

#### 3.7.1 inner join（join）
sql:
```
select
    a.name1 aname1,
    a.name2 aname2,
    b.name1 aname1,
    b.name2 aname2
from
    t_a a inner join
    t_b b on
    a.name1 = b.name1;
```
结果：
```
+---------+---------+---------+---------+--+
| aname1  | aname2  | aname1  | aname2  |
+---------+---------+---------+---------+--+
| a       | 1       | a       | xx      |
| b       | 2       | b       | yy      |
| d       | 4       | d       | zz      |
+---------+---------+---------+---------+--+
```

#### 3.7.2 left outer join（left join）

sql:
```
select
    a.name1 aname1,
    a.name2 aname2,
    b.name1 aname1,
    b.name2 aname2
from
    t_a a left outer join
    t_b b on
    a.name1 = b.name1;
```
结果：
```
+---------+---------+---------+---------+--+
| aname1  | aname2  | aname1  | aname2  |
+---------+---------+---------+---------+--+
| a       | 1       | a       | xx      |
| b       | 2       | b       | yy      |
| c       | 3       | NULL    | NULL    |
| d       | 4       | d       | zz      |
+---------+---------+---------+---------+--+
```

#### 3.7.3 right outer join（right join）

sql:
```
select
    a.name1 aname1,
    a.name2 aname2,
    b.name1 aname1,
    b.name2 aname2
from
    t_a a right outer join
    t_b b on
    a.name1 = b.name1;
```
结果：
```
+---------+---------+---------+---------+--+
| aname1  | aname2  | aname1  | aname2  |
+---------+---------+---------+---------+--+
| a       | 1       | a       | xx      |
| b       | 2       | b       | yy      |
| d       | 4       | d       | zz      |
| NULL    | NULL    | e       | pp      |
+---------+---------+---------+---------+--+
```

#### 3.7.5 full outer join（full join）

sql:
```
select
    a.name1 aname1,
    a.name2 aname2,
    b.name1 aname1,
    b.name2 aname2
from
    t_a a full outer join
    t_b b on
    a.name1 = b.name1;
```
结果：
```
+---------+---------+---------+---------+--+
| aname1  | aname2  | aname1  | aname2  |
+---------+---------+---------+---------+--+
| a       | 1       | a       | xx      |
| b       | 2       | b       | yy      |
| c       | 3       | NULL    | NULL    |
| d       | 4       | d       | zz      |
| NULL    | NULL    | e       | pp      |
+---------+---------+---------+---------+--+
```

#### 3.7.6	left semi join

sql:
```
select
    a.name1 aname1,
    a.name2 aname2
from
    t_a a left semi join
    t_b b on
    a.name1 = b.name1;
```

结果：
```
+---------+---------+--+
| aname1  | aname2  |
+---------+---------+--+
| a       | 1       |
| b       | 2       |
| d       | 4       |
+---------+---------+--+
```
注意： left semi join的 select子句中，不能有右表的字段

### 3.8 group by分组聚合
注意：   
- ==一旦有group by子句，那么，在select子句中就不能有 （分组字段，聚合函数） 以外的字段==   
- ==么where必须写在group by的前面，group by后面的条件只能用having==
- where过滤不满足条件的数据
- 用聚合函数和group by进行数据运算聚合，得到聚合结果
- 用having条件过滤掉聚合结果中不满足条件的数据



1. upper():     
将字段变为大写
```sql
select ip, upper(url),time from t_pv_log;
```
```
+------------+-----------------------+----------------------+--+
|     ip     |          _c1          |         time         |
+------------+-----------------------+----------------------+--+
| 10.8.9.41  | HTTP://WWW.BAIDU.COM  | 2019-02-24 22:58:22  |
| 10.8.9.42  | HTTP://WWW.BAIDU.COM  | 2019-02-24 22:48:22  |
| 10.8.9.43  | HTTP://WWW.BAIDU.COM  | 2019-02-24 22:52:22  |
| 10.8.9.44  | HTTP://WWW.BAIDU.COM  | 2019-02-24 22:53:22  |
+------------+-----------------------+----------------------+--+
```

2. 分组 
```
select url,count(id) nub from t_pv_log group by url;
```
```
+-------------------------+------+--+
|           url           | nub  |
+-------------------------+------+--+
| http://www.baidu.com    | 15   |
| http://www.baidu.com/a  | 6    |
| http://www.baidu.com/b  | 2    |
| http://www.baidu.com/c  | 6    |
| http://www.baidu.com/d  | 1    |
+-------------------------+------+--+

```

2. 分组聚合 
```
select url,max(time) nub from t_pv_log group by url;
```
```
+-------------------------+----------------------+--+
|           url           |         nub          |
+-------------------------+----------------------+--+
| http://www.baidu.com    | 2019-02-24 22:59:26  |
| http://www.baidu.com/a  | 2019-02-24 22:58:27  |
| http://www.baidu.com/b  | 2019-02-24 22:58:25  |
| http://www.baidu.com/c  | 2019-02-24 22:59:26  |
| http://www.baidu.com/d  | 2019-02-24 22:53:22  |
+-------------------------+----------------------+--+

```

## 4 hive函数使用

都是对某一个字段的转换  
[hive函数手册](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-Built-inTable-GeneratingFunctions(UDTF))

```
select substring ("abcdefghigk",1,3);//是从1开始 所有0或1 都是第一个
```
```
+------+--+
| _c0  |
+------+--+
| abc  |
+------+--+
```

### 4.1	常用内置函数
#### 4.1.1	类型转换函数

```
select cast("5" as int);
select cast("2019-02-25" as date) ;
select cast(current_timestamp as date);
//current_timestamp 为当前时间    
//unix_timestamp() 毫秒时间

```

#### 4.1.2	数学运算函数
- round() 随机数
```
select round(5.4) ;
select round(5.1345,3) ;
```
- 向上取整
```
select ceil(5.4) ;
```
- 向下取整
```
select floor(5.4); 
```
- 取绝对值
```
select abs(-5.4);  
```
- 取最大值
```
select greatest(3,5,6) ;  
```
- 取最小值
```
select least(3,5,6);
```


#### 4.1.3	字符串函数
- 截取子串
```
substr(string, int start)  
substring(string, int start)
示例：select substr("abcdefg",2);//bcdefg

substr(string, int start, int len) 
substring(string, int start, int len)
示例：select substr("abcdefg",2,3); //bcd
```

- 拼接字符串
```
concat(string A, string B...) 
concat_ws(string SEP, string A, string B...)
示例：select concat("ab","xy"); //abxy
select concat_ws(".","192","168","33","44");//192.168.33.44
```

- 求长度
```
length(string A)
示例：select length("192.168.33.44");//13
```

- 分割
```
split(string str, string pat)
示例： select split("192.168.33.44","\\.");
```
- 转大写
```
upper(string str) 
```

#### 4.1.4 时间函数
- 取当前时间的毫秒数时间戳 

```
select unix_timestamp();
```


- unix时间戳转字符串

```
from_unixtime(bigint unixtime[, string format])
示例：select from_unixtime(unix_timestamp());
select from_unixtime(unix_timestamp(),"yyyy/MM/dd HH:mm:ss");
```

- 字符串转unix时间戳

```
unix_timestamp(string date, string pattern)
示例： select unix_timestamp("2017-08-10 17:50:30");
select unix_timestamp("2017/08/10 17:50:30","yyyy/MM/dd HH:mm:ss");
```
   
- 将字符串转成日期date

```
select to_date("2017-09-17 16:58:32");
```