# 1. 为什么使用spark
    1. Hadoop：两步计算，磁盘存储
    2. Spark：多步计算，内存存储    
Spark是MapReduce的替代方案，且兼容HDFS，hive

# 2. 安装spark
1. 下载[spark](https://spark.apache.org/) 注意hadoop版本
2. 解压
3. 修改配置文件
    1. 修改`spark-env.sh`       
    ```
    export JAVA_HOME=/home/chenpeng/apps/jdk/jdk8/
    export SPARK_MASTER_HOST=vm04
    export SPARK_MASTER_PORT=7077
    #export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=vm04,vm05 -Dspark.deploy.zookeeper.dir=/spark"
    ```

    2. 修改`slaves` 
    `mv slaves.template slaves`
    ```
    vm04
    vm05
    ```
    

4. 可以配高可用
```
export JAVA_HOME=/home/chenpeng/apps/jdk/jdk8/
#export SPARK_MASTER_HOST=vm04
#export SPARK_MASTER_PORT=7077
export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=vm04:2181,vm05:2181 -Dspark.deploy.zookeeper.dir=/spark"
```
注意要先配置Zk ，而且要先启动       

在node1上执行sbin/start-all.sh脚本，然后在node2上执行sbin/start-master.sh启动第二个Master
# 3. demo   
提交一个spark应用到集群中运行       
```
bin/spark-submit --master spark://vm04:7077 --class org.apache.spark.examples.SparkPi examples/jars/spark-examples_2.12-2.4.2.jar 100
```

参数说明：      
--master spark://vm04:7077 指定Master的地址      
--executor-memory 2g 指定每个worker可用内存为2G         
--total-executor-cores 2 指定整个集群使用的cup核数为2个     

# 4.原理
![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190508173554.png)
![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190508213424.png)
![RDD排序](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190509163933.png)
## 4.1. Yarn和Spark的StandAlone调度模式对比      

Yarn|Spark|描述
---|---|---
ResouceManager|Master|管理子节点，资源调度，接受任务请求
NodeMangr|Worker |管理当前子节点，并管理子进程
YarnChild|Executor|运行真正的计算逻辑的
Client|SparkSubmit|提交app，管理该任务的Executor，并将Task提交到集群（Executor）

# 5. 使用
## 5.1. Spark Shell
> 是一个交互式的命令行，里面可以写spark程序，方便学习和测试，也是一个客户端，可以用于提交spark应用程序

1. 启动     

没有指定master的地址，即为spark的local模式运行  
```
bin/spark-shell
```

集群模式运行，会将任务提交到集群，开始时：sparksubmit（客户端）要连接Master，申请计算资源（内存和核数），Master进行资源调度（Worker启动Executor），在准备工作时，这些进程已经创建好了
```
bin/spark-shell --master spark://vm04：7077
```
2. wordcount    
    1. 启动hdfs
    2. 在scala中输入    
    ```
    sc.textFile("hdfs://vm04:9000/wordcount").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
    ```
    
## 5.2 scala方式    
1. code：   
```
<properties>
        <maven.compiler.source>1.8</maven.compiler.source>
        <maven.compiler.target>1.8</maven.compiler.target>
        <scala.version>2.12.8</scala.version>
        <spark.version>2.4.2</spark.version>
        <hadoop.version>2.9.2</hadoop.version>
        <encoding>UTF-8</encoding>
    </properties>


    <dependencies>
        <!-- 导入scala的依赖 -->
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>${scala.version}</version>
        </dependency>

        <!-- 导入spark的依赖 -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.12</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <!-- 指定hadoop-client API的版本 -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>${hadoop.version}</version>
        </dependency>

    </dependencies>

    <build>
        <pluginManagement>
            <plugins>
                <!-- 编译scala的插件 -->
                <plugin>
                    <groupId>net.alchim31.maven</groupId>
                    <artifactId>scala-maven-plugin</artifactId>
                    <version>3.2.2</version>
                </plugin>
                <!-- 编译java的插件 -->
                <plugin>
                    <groupId>org.apache.maven.plugins</groupId>
                    <artifactId>maven-compiler-plugin</artifactId>
                    <version>3.5.1</version>
                </plugin>
            </plugins>
        </pluginManagement>
        <plugins>
            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <executions>
                    <execution>
                        <id>scala-compile-first</id>
                        <phase>process-resources</phase>
                        <goals>
                            <goal>add-source</goal>
                            <goal>compile</goal>
                        </goals>
                    </execution>
                    <execution>
                        <id>scala-test-compile</id>
                        <phase>process-test-resources</phase>
                        <goals>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>

            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <executions>
                    <execution>
                        <phase>compile</phase>
                        <goals>
                            <goal>compile</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>


            <!-- 打jar插件 -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-shade-plugin</artifactId>
                <version>2.4.3</version>
                <executions>
                    <execution>
                        <phase>package</phase>
                        <goals>
                            <goal>shade</goal>
                        </goals>
                        <configuration>
                            <filters>
                                <filter>
                                    <artifact>*:*</artifact>
                                    <excludes>
                                        <exclude>META-INF/*.SF</exclude>
                                        <exclude>META-INF/*.DSA</exclude>
                                        <exclude>META-INF/*.RSA</exclude>
                                    </excludes>
                                </filter>
                            </filters>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
```

```
object ScalaWordCount {
  def main(args: Array[String]): Unit = {
    //创建spark配置
    val conf = new SparkConf()
    conf.setAppName("ScalaWordCount")
    //创建spark执行的入口
    val sc = new SparkContext(conf)
    val lines:RDD[String] = sc.textFile(args(0))
    //切分压片
    val words: RDD[String] = lines.flatMap(_.split(" "))
    //将单词和为一组
    val wordAndOne: RDD[(String, Int)] = words.map((_,1))
    //按key进行聚合
    val reduced: RDD[(String, Int)] = wordAndOne.reduceByKey(_+_)
    //排序
    val sorted = reduced.sortBy(_._2, false)

    //将结果保存到HDFS中
    sorted.saveAsTextFile(args(1))

    //释放资源
    sc.stop()
  }
}
```

2. [提交](#提交)

## 5.3 java方式
```
public class JavaWordCount {
    public static void main(String[] args) {
        SparkConf sparkConf = new SparkConf().setAppName("JavaWordCount");
        //创建sparkContext
        JavaSparkContext jsc = new JavaSparkContext(sparkConf);
        //指定以后从哪里读取数据
        JavaRDD<String> lines = jsc.textFile(args[0]);

        //切分压平 String插入 String传出
        JavaRDD<String> words = lines.flatMap(new FlatMapFunction<String, String>() {
            @Override
            public Iterator<String> call(String line) throws Exception {
                return Arrays.asList(line.split(" ")).iterator();
            }
        });

        //将单词和1组合在一起 String传入 String传出key String传出value
        JavaPairRDD<String, Integer> wordAndOne =
                words.mapToPair(new PairFunction<String, String, Integer>() {
            @Override
            public Tuple2<String, Integer> call(String word) throws Exception {
                return new Tuple2<>(word, 1);
            }
        });

        //聚合 第一个Integer相同key的value 第二个Integer相同key的value 第三个Integer返回值
        JavaPairRDD<String, Integer> reduce = wordAndOne.reduceByKey(new Function2<Integer, Integer, Integer>() {
            @Override
            public Integer call(Integer integer, Integer integer2) throws Exception {
                return integer + integer2;
            }
        });

        //交换key value顺序
        JavaPairRDD<Integer, String> swaped = reduce.mapToPair(new PairFunction<Tuple2<String, Integer>, Integer, String>() {
            @Override
            public Tuple2<Integer, String> call(Tuple2<String, Integer> tp) throws Exception {
                //return new Tuple2<>(tp._2,tp._1);
                return tp.swap();
            }
        });

        //排序
        JavaPairRDD<Integer, String> sorted = swaped.sortByKey(false);

        JavaPairRDD<String, Integer> reslut = sorted.mapToPair(new PairFunction<Tuple2<Integer, String>, String, Integer>() {
            @Override
            public Tuple2<String, Integer> call(Tuple2<Integer, String> tp) throws Exception {
                return tp.swap();
            }
        });

        //将数据保存
        reslut.saveAsTextFile(args[1]);

        //释放
        jsc.stop();



    }
}
```
## 5.4 JavaLambda方式
```
public class JavaLambdaWordCount {
    public static void main(String[] args) {
        SparkConf conf = new SparkConf().setAppName("JavaLambdaWordCount").setMaster("local[4]");

        JavaSparkContext  jsc = new JavaSparkContext(conf);

        //设定读取数据的位置
        JavaRDD<String> lines = jsc.textFile(args[0]);

        //切分压平
        //JavaRDD<Object> words = lines.flatMap(line -> Arrays.asList(line.split(" ")).iterator());
        JavaRDD<String> words = lines.flatMap(new FlatMapFunction<String, String>() {
            @Override
            public Iterator<String> call(String s) throws Exception {
                return Arrays.asList(s.split(" ")).iterator();
            }
        });
        //分组为单词和1
        JavaPairRDD<Object, Integer> wordAndOne = words.mapToPair(word -> new Tuple2<>(word, 1));


        //聚合
        JavaPairRDD<Object, Integer> reduced = wordAndOne.reduceByKey((m, n) -> m + n);

        //翻转
        JavaPairRDD<Integer, Object> fWrod = reduced.mapToPair(w -> w.swap());

        //排序
        JavaPairRDD<Integer, Object> shortMap = fWrod.sortByKey(false);

        //翻转
        JavaPairRDD<Object, Integer> coulse = shortMap.mapToPair(w -> w.swap());

        coulse.saveAsTextFile(args[1]);

        jsc.close();


    }
}
```

## 5.5 Windows环境调试
1. 配置Windows环境下的hadoop    
![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190508173656.png)


2. 配置Program arguments设置传入的值 args
![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190508173745.png) 


3. 点击运行即可


## 5.6 总结
1. 设置SparkConf    
2. 创建JavaSparkContext
3. 设置输入数据的位置`textFile()`
```
JavaRDD<String> lines = jsc.textFile(args[0]);
```

4. 将每行数据切片`flatMap()`
```java
//new FlatMapFunction<String, String>() {...} 中的第一个String 为传入数据类型，第二个String 为输出的数据类型
JavaRDD<String> words = lines.flatMap(new FlatMapFunction<String, String>() {
    @Override
    public Iterator<String> call(String line) throws Exception {
        return Arrays.asList(line.split(" ")).iterator();
    }
});
```

5. 创建 类似map 的数据类型 用于计数`mapToPair()`    
```java
// new PairFunction<String, String, Integer>() {...}中第一个String 为传入的数据类型，第二个String 为输出的Key，第三个Integer 为输出Value 
// 第四步将数据切为片之后 次方法将他们全部变为（Word，1）的形式，用于后续统计
JavaPairRDD<String, Integer> wordAndOne =
        words.mapToPair(new PairFunction<String, String, Integer>() {
    @Override
    public Tuple2<String, Integer> call(String word) throws Exception {
        return new Tuple2<>(word, 1);
    }
});
```

6. 聚合数据
```java
// new Function2<Integer, Integer, Integer>() {...}中第一个Integer为传入key相同的第一个数据的value，第二个Intger为第二个数据的value，第三个Integer为输出数据类型
//次步将所有键值对数据 key相同数据的value相加
JavaPairRDD<String, Integer> reduce = wordAndOne.reduceByKey(new Function2<Integer, Integer, Integer>() {
    @Override
    public Integer call(Integer integer, Integer integer2) throws Exception {
        return integer + integer2;
    }
});
```


7. 交换key value    
```java
JavaPairRDD<Integer, String> swaped = reduce.mapToPair(new PairFunction<Tuple2<String, Integer>, Integer, String>() {
    @Override
    public Tuple2<Integer, String> call(Tuple2<String, Integer> tp) throws Exception {
        return tp.swap();
    }
});
```


8. 排序
```
JavaPairRDD<Integer, String> sorted = swaped.sortByKey(false);
```

9.将数据保存
```
reslut.saveAsTextFile(args[1]);
```

10. 释放资源
```
jsc.stop();
``` 


<span id="提交"></span>
## 提交
1. hdfs启动
```
/home/chenpeng/apps/bigdata/hadoop/hadoop-2.9.2/sbin/start-dfs.sh
```
2. spark启动
```
/home/chenpeng/apps/bigdata/spark/spark-2.4.2-bin-hadoop2.7/sbin/start-all.sh
```
3. idea导出jar包
![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190508173839.png)

4. 提交
```
/home/chenpeng/apps/bigdata/spark/spark-2.4.2-bin-hadoop2.7/bin/spark-submit --master spark://vm04:7077 --class com.looc.spark.scalaDemo1.ScalaWordCount ./original-SparkDemo-1.0-SNAPSHOT.jar hdfs://vm04:9000/wc hdfs://vm04:9000/wc0506

# SPARK_HOME/bin/spark-submit --master spark://（master所在服务器：端口） --class （入口的jar包中的位置） （jar包的位置） （需要处理的文件路径）（处理完之后的路径）
```
5. 执行过程
![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190509195131.png)


