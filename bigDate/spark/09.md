1. 只需要启动hdfs，和yarn，然后在一台有安装过spark的机器上提交任务



2. 需要修改环境变量
```
export HADOOP_CONF_DIR=/usr/local/hadoop-2.7.3/etc/hadoop
```

3. 在spark-env.sh中添加

export SPARK_YARN_USER_ENV="CLASSPATH=hadoop-2.3.0/etc/hadoop"（修改成自己的hadoop路径）


# 1. cluster模式

1. demo
```
./spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster --driver-memory 1g --executor-memory 1g --executor-cores 2 --queue default ../examples/jars/spark-examples_2.12-2.4.2.jar 10
```


