[TOC]
# 1. 简介
1. RDD 弹性分布式数据集，RDD里面并不存储真正要计算的数据，对RDD进行操作，它会在Driver端转换成Task，下发到Executor计算分散在多台集群上的数据      
2. RDD 是一个代理，对代理进行操作，它会生成Task，帮你计算    
3. 操作这个代理，就像操作本地集合一样，不用关心任务调度，容错
4. RDD的算子分为两类，一类是Transformation（lazy）,一类是Action（触发任务执行）

# 2 创建RDD的方式
1. 用过外部存储系统创建RDD （HDFS）
2. 将Driver的Scala集合通过并行化的方式编程RDD（试验，测验）
3. 调用一个已经存在了的RDD 的Tranformation，会生成一个新的RDD
![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190508173913.png)





```
#常用Transformation(即转换，延迟加载)
#通过并行化scala集合创建RDD
val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8))
#查看该rdd的分区数量
rdd1.partitions.length


val rdd1 = sc.parallelize(List(5,6,4,7,3,8,2,9,1,10))
val rdd2 = sc.parallelize(List(5,6,4,7,3,8,2,9,1,10)).map(_*2).sortBy(x=>x,true)
val rdd3 = rdd2.filter(_>10)
val rdd2 = sc.parallelize(List(5,6,4,7,3,8,2,9,1,10)).map(_*2).sortBy(x=>x+"",true)
val rdd2 = sc.parallelize(List(5,6,4,7,3,8,2,9,1,10)).map(_*2).sortBy(x=>x.toString,true)


val rdd4 = sc.parallelize(Array("a b c", "d e f", "h i j"))
rdd4.flatMap(_.split(' ')).collect

val rdd5 = sc.parallelize(List(List("a b c", "a b b"),List("e f g", "a f g"), List("h i j", "a a b")))


List("a b c", "a b b") =List("a","b",))

rdd5.flatMap(_.flatMap(_.split(" "))).collect

#union求并集，注意类型要一致
val rdd6 = sc.parallelize(List(5,6,4,7))
val rdd7 = sc.parallelize(List(1,2,3,4))
val rdd8 = rdd6.union(rdd7)
rdd8.distinct.sortBy(x=>x).collect

#intersection求交集
val rdd9 = rdd6.intersection(rdd7)


val rdd1 = sc.parallelize(List(("tom", 1), ("jerry", 2), ("kitty", 3)))
val rdd2 = sc.parallelize(List(("jerry", 9), ("tom", 8), ("shuke", 7), ("tom", 2)))

#join(连接)
val rdd3 = rdd1.join(rdd2)
val rdd3 = rdd1.leftOuterJoin(rdd2)
val rdd3 = rdd1.rightOuterJoin(rdd2)


#groupByKey
val rdd3 = rdd1 union rdd2
rdd3.groupByKey
//(tom,CompactBuffer(1, 8, 2))
rdd3.groupByKey.map(x=>(x._1,x._2.sum))
groupByKey.mapValues(_.sum).collect
Array((tom,CompactBuffer(1, 8, 2)), (jerry,CompactBuffer(9, 2)), (shuke,CompactBuffer(7)), (kitty,CompactBuffer(3)))


#WordCount
sc.textFile("/root/words.txt").flatMap(x=>x.split(" ")).map((_,1)).reduceByKey(_+_).sortBy(_._2,false).collect
sc.textFile("/root/words.txt").flatMap(x=>x.split(" ")).map((_,1)).groupByKey.map(t=>(t._1, t._2.sum)).collect

#cogroup
val rdd1 = sc.parallelize(List(("tom", 1), ("tom", 2), ("jerry", 3), ("kitty", 2)))
val rdd2 = sc.parallelize(List(("jerry", 2), ("tom", 1), ("shuke", 2)))
val rdd3 = rdd1.cogroup(rdd2)
val rdd4 = rdd3.map(t=>(t._1, t._2._1.sum + t._2._2.sum))

#cartesian笛卡尔积
val rdd1 = sc.parallelize(List("tom", "jerry"))
val rdd2 = sc.parallelize(List("tom", "kitty", "shuke"))
val rdd3 = rdd1.cartesian(rdd2)

###################################################################################################

#spark action
val rdd1 = sc.parallelize(List(1,2,3,4,5), 2)

#collect
rdd1.collect

#reduce
val r = rdd1.reduce(_+_)

#count
rdd1.count

#top
rdd1.top(2)

#take
rdd1.take(2)

#first(similer to take(1))
rdd1.first

#takeOrdered
rdd1.takeOrdered(3)
```

# 3. RDD的Transformation的特点
1. lazy不会立即执行
2. 会生成新的RDD
3. 记录着信息的源数据信息，（数据在什么位置，调了那些方法，传了什么函数）

# 4. 算子
## 4.1 map ———— map是对RDD中的每个元素都执行一个指定的函数来产生一个新的RDD。
map是对RDD中的每个元素都执行一个指定的函数来产生一个新的RDD。 任何原RDD中的元素在新RDD中都有且只有一个元素与之对应。
![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190508174014.png)

## 4.2 mapPartitions ————— 把每个分区中的内容作为整体来处理的    
mapPartitions是map的一个变种。map的输入函数是应用于RDD中每个元素，
而mapPartitions的输入函数是应用于每个分区，也就是把每个分区中的内容作为整体来处理的。
![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190508174046.png)

## 4.3 mapValues 函数       
mapValues顾名思义就是输入函数应用于RDD中Kev-Value的Value，原RDD中的Key保持不变，与新的Value一起组成新的RDD中的元素。
因此，该函数只适用于元素为KV对的RDD。
![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190508174118.png)  
## 4.4 mapPartitionsWithIndex
mapPartitionsWithIndex是函数作用同mapPartitions，不过提供了两个参数，第一个参数为分区的索引。
测试结果: [0|3, 1|12]
![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190508174141.png)
## 4.5 flatMap 函数 ————  而原RDD中的元素经flatmap处理后可生成多个元素来构建新RDD
与map类似，区别是原RDD中的元素经map处理后只能生成一个元素，而原RDD中的元素经flatmap处理后可生成多个元素来构建新RDD。
举例：对原RDD中的每个元素x产生y个元素（从1到y，y为元素x的值）
![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190508174202.png)


## 4.6 aggregate
- aggregate 需要传入三个值        
- aggregate(Integer zeroValue,Function1 f1,Function2 f2);        
- zeroValue 为初始值      
- f1 为第一个对每个分组进行操作       
- f2 对f1的每个处理的结果进行操作   
- 对于初始值两次都使用了

![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190508174227.png)  


## 4.7 aggregateByKey   
- 按key来划分
- 第一个函数 每个分组进行聚合
- 第二个函数 对每组的结构在进行聚合

![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190508174258.png)


## 4.8 reduce、fold
- 元素两两传入，进过函数产生一个新的值
- 拿着新的值再和下一个元素传入函数

![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190508174323.png)


## 4.9 reduceByKey
- 对元素的KV键值对的RDD中Key相同的Value进行reduce   
- Key相同的多个Value被reduce成一个值，然后组成一个新的RDD   

![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190508181850.png)


## 4.10 cartesian ———— 笛卡尔积运算 
- 笛卡尔积运算  
![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190508182952.png)

## 4.11 glom ———— 将RDD中的每个函数分成多个数组
- 将RDD中的每个函数分成多个数组
![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190508195000.png)


## 4.12 subtract ———— 对两个RDD中的数据进行减法操作
- 对两个RDD中的数据进行减法操作

![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190508200617.png)

## 4.13 sample ————— 采样
- sample 中第二个参数为采样率
- taskSample 中第二个参数为采样个数
- 两个中的第一个参数都为是否 放回抽样

![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190508201019.png)


## 4.14 combineByKey 
![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190509134434.png)

## 4.15 countByKey  ———— 按key出现的次数来统计

## 4.16 filterByRange ———— 按key指定的范围过滤 包含边界

## 4.17 flatMapValues ———— 对Value进行flatMap

## 4.18 tack ———— 取前几个

## 4.19 zip ———— 拉链操作


# 5. 区分Transformation和Action
- aggregateByKey   是Transformation
- reduceByKey      是Transformation
- filter           是Transformation
- flatMap			 是Transformation
- map              是Transformation
- mapPartition     是Transformation
- mapPartitionWithIndex 是Transformation


- collect          是Action
- aggregate        是Action
- saveAsTextFile   是Action
- foreach          是Action
- foreachPartition 是Action



# 6. foreach 、 foreachPartition
- 是Action
demo:   
```
rdd.foreach(f -> System.out.Println(f));
```
实际输出是在task的机器上   

```
rdd.foreachPartitionAsync(f -> f.foreach(x -> System.out.Println(x)));
```
数据库最好用foreachPartitionAsync   分区迭代

# 7. 自定义分区器
![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190509182938.png)

# 8. broadcast 广播
```
//广播
//因为Task是在Driver端产生的， 广播变量是随着Task被发送到Executor中的
//broadcast.value() 可以取到值
Broadcast<List<Tuple2<Tuple2<Long, Long>, String>>> broadcast = jsc.broadcast(ruler);
```
```
//从广播拿到规则
List<Tuple2<Tuple2<Long, Long>, String>> rulers = broadcast.value();

```


# 8. cache
```
rdd.cache();
```
- cache 到内存（标记Cache的RDD以后反复使用才用cache）
- 释放 是unpersist(true)  其中的true是代表等待所有使用完后释放 ，如果是false就将是异步释放

- 要求计算速度很快
- 集群的资源足够大
- cache的数据会多次触发Action
- 最好缩小分为再cache到内存


# 9. checkpoint
1. 迭代计算，要求保证数据安全
2. 对熟读要求不高，相对于cache到内存进行比较
3. 将中间数据保存到hdfs

- 先设置checkpoint目录
- 需要保存数据的那个地方调用.checkpoint即可 
- 后序的计算，直接从中间结构来得到数据 进行计算
- 建议先.cache() 然后再checkpoint  预防重复读
- 如果cache 与 checkpoint同时存在，优先使用cache
```
jsc.setCheckpointDir("hdfs://vm04:9000");

rdd.checkpoint();
```


# 10. 总结
RDD中存如果存储的是KV，Shuffle是会有一个分区器，模式Hash partitioner        
如果是读取HDFS中的数据，那么就会有一个最佳位置



demo:   
```
package com.looc.d02rdd;

import org.apache.commons.collections.IteratorUtils;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import scala.Tuple2;

import java.util.*;

/**
 * @author chenPeng
 * @version 1.0.0
 * @ClassName RddDemo2.java
 * @Description TODO
 *
 *  * 2.求每个学科中最受欢迎老师的top3
 *
 *
 * @createTime 2019年05月09日 01:27:00
 */
public class RddDemo2 {
    private static final String FILE_PATH = "H:\\javaCode\\ideaCode\\Android\\SparkDemo\\file\\teacher.log";

    public static void main(String[] args) {
        SparkConf conf = new SparkConf().setAppName("JSpark").setMaster("local[4]");
        JavaSparkContext jsc = new JavaSparkContext(conf);

        JavaRDD<String> rdd = jsc.textFile(FILE_PATH);

        System.out.println("========================================");
        System.out.println("==================demo1=================");
        demo1(rdd);

        System.out.println("========================================");



        System.out.println("========================================");
        System.out.println("==================demo2=================");
        demo2(rdd);
        System.out.println("========================================");




        System.out.println("========================================");
        System.out.println("==================demo3=================");
        demo3(rdd);
        System.out.println("========================================");


        jsc.stop();
    }

    /**
     * @Author chenpeng
     * @Description //
     * TODO
     *  求每个学科中最受欢迎老师的top3
     *  如果数据太大 数据将会爆表
     *  可以使用方法二 也可以采用在 对每个类别进行操作 中 改为指定大小的List 自己来排序 只放需要的个数
     * @Date 1:32 
     * @Param [rdd]
     * @return void
     **/
    private static void demo1(JavaRDD<String> rdd) {
        //得到课程和老师姓名的键值对((学科，姓名)，1)
        JavaPairRDD<Tuple2<String, String>, Integer> rdd1 = rdd.mapToPair(f -> {
            String[] split = f.split("/");
            Tuple2<String, String> tuple2 = new Tuple2<>(split[2].split("[.]")[0], split[3]);
            return new Tuple2<>(tuple2, 1);
        });

        //合并数据((学科，姓名)，N)
        JavaPairRDD<Tuple2<String, String>, Integer> byKey = rdd1.reduceByKey((a, b) -> a + b);

        //数据分组 按学科来分类
        JavaPairRDD<String, Iterable<Tuple2<Tuple2<String, String>, Integer>>> rdd2 =
                byKey.groupBy(f -> f._1._1);


        //对每个类别进行操作
        JavaPairRDD<String, List<Tuple2<Tuple2<String, String>, Integer>>> rdd3 = rdd2.mapValues(f -> {

            List<Tuple2<Tuple2<String, String>, Integer>> list = IteratorUtils.toList(f.iterator());
            list.sort(new Comparator<Tuple2<Tuple2<String, String>, Integer>>() {
                @Override
                public int compare(Tuple2<Tuple2<String, String>, Integer> o1, Tuple2<Tuple2<String, String>, Integer> o2) {
                    return o2._2 - o1._2;
                }
            });


            return list.size()>3? list.subList(0,3):list.subList(0,list.size());
        });

        rdd3.foreach(f-> System.out.println(f._1+" "+f._2));
    }


    /**
     * @Author chenpeng
     * @Description
     * //TODO
     *      求每个学科中最受欢迎老师的top3（至少用2到三种方式实现）
     *      使用filter 进行多次提交
     * @Date 1:32
     * @Param [rdd]
     * @return void
     **/
    private static void demo2(JavaRDD<String> rdd) {
        //预先获取到分类
        String[] strs = {"bigdata","php","javaee"};


        //得到课程和老师姓名的键值对((学科，姓名)，1)
        JavaPairRDD<Tuple2<String, String>, Integer> rdd1 = rdd.mapToPair(f -> {
            String[] split = f.split("/");
            return new Tuple2<>(
                    new Tuple2<>(split[2].split("[.]")[0], split[3]),
                    1);
        });

        //聚合 ((学科，姓名)，N)
        JavaPairRDD<Tuple2<String, String>, Integer> rdd2 = rdd1.reduceByKey((a, b) -> a + b);

        for (String str : strs) {
            JavaPairRDD<Tuple2<String, String>, Integer> filter = rdd2.filter(f -> f._1._1.equals(str));

            List<Tuple2<Tuple2<String, String>, Integer>> list =
                    filter.mapToPair(f -> f.swap()).sortByKey(false).mapToPair(f -> f.swap()).take(3);

            list.forEach(f->System.out.println(f));
        }



    }


    /**
     * @Author chenpeng
     * @Description
     * //TODO
     *      求每个学科中最受欢迎老师的top3（至少用2到三种方式实现）
     *      自定义分区器
     * @Date 1:32
     * @Param [rdd]
     * @return void
     **/
    /*private static void demo3(JavaRDD<String> rdd) {
        //得到课程和老师姓名的键值对((学科，姓名)，1)
        JavaPairRDD<Tuple2<String, String>, Integer> rdd1 = rdd.mapToPair(f -> {
            String[] split = f.split("/");
            return new Tuple2<>(
                    new Tuple2<>(split[2].split("[.]")[0], split[3]),
                    1);
        });

        //聚合 ((学科，姓名)，N)
        JavaPairRDD<Tuple2<String, String>, Integer> rdd2 = rdd1.reduceByKey((a, b) -> a + b);

        //得到不同的学科
        List<String> collect = rdd2.map(f -> f._1._1).distinct().collect();

        JavaPairRDD<Tuple2<String, String>, Integer> rdd3 = rdd2.partitionBy(new Partitioner() {
            @Override
            public int getPartition(Object key) {
                Tuple2<Tuple2<String, String>, Integer> k = (Tuple2<Tuple2<String, String>, Integer>) key;
                for (int i = 0; i < collect.size(); i++) {
                    if (k._1._1.equals(collect.get(i))) {
                        return i;
                    }
                }
                return 0;
            }

            @Override
            public int numPartitions() {
                return collect.size();
            }
        });


        rdd3.mapPartitions(f-> {
            f.
        })

    }*/
}

```


demo2:
```
package com.looc.d03ip;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.broadcast.Broadcast;
import scala.Tuple2;

import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.PreparedStatement;
import java.util.List;

/**
 * @author chenPeng
 * @version 1.0.0
 * @ClassName SparkIpAnalysi.java
 * @Description TODO
 * @createTime 2019年05月10日 23:38:00
 */
public class SparkIpAnalysis {
    public static void main(String[] args) {
        SparkConf conf = new SparkConf().setAppName("JavaSpark").setMaster("local[4]");
        JavaSparkContext jsc = new JavaSparkContext(conf);


        //读取ip规则到Driver
        List<Tuple2<Tuple2<Long, Long>, String>> ruler = MyUtil.readRuler(args[0],jsc);

        //广播
        //因为Task是在Driver端产生的， 广播变量是随着Task被发送到Executor中的
        //broadcast.value() 可以取到值
        Broadcast<List<Tuple2<Tuple2<Long, Long>, String>>> broadcast = jsc.broadcast(ruler);

        //读取访问日志
        JavaRDD<String> logs = jsc.textFile(args[1]);
        JavaPairRDD<String, Integer> logCount = logs.mapToPair(line -> {
            String[] word = line.split("[|]");

            //从广播拿到规则
            List<Tuple2<Tuple2<Long, Long>, String>> rulers = broadcast.value();

            //与规则进行对比得到地区
            String city = MyUtil.twoPoints(rulers, IpConversion.ipConversion(word[1]));

            return new Tuple2<>(city, 1);
        });

        //聚合统计出每个城市的数量
        JavaPairRDD<String, Integer> result = logCount.reduceByKey((a, b) -> a + b);

       /* //打印输出
        result.foreach(f->System.out.println(f));*/

        //写入数据库
        result.foreachPartition(f->{
            Connection connection = DriverManager.getConnection(
                    "jdbc:mysql://localhost:3306/bigdata?charatorEncoding=utf-8","root","root");
            PreparedStatement pSta = connection.prepareStatement("INSERT INTO citylog(city, nub)  value (?,?)");
            while (f.hasNext()){
                Tuple2<String, Integer> next = f.next();
                pSta.setString(1, next._1);
                pSta.setInt(2, next._2);
                pSta.executeUpdate();
            }
            pSta.close();
            connection.close();
        });

        jsc.stop();
    }
}




package com.looc.d03ip;

/**
 * @author chenPeng
 * @version 1.0.0
 * @ClassName IpConversion.java
 * @Description TODO 将点分十进制转换为十进制
 * @createTime 2019年05月10日 23:26:00
 */
public class IpConversion {
    public static Long ipConversion(String ip){
        String[] ipArr = ip.split("[.]");
        Long sc = 0L;

        for (int i = 0; i < ipArr.length; i++) {
            int temp = Integer.parseInt(ipArr[i]) << 8*(ipArr.length-i-1);
            sc = sc | temp;
        }
        return sc;
    }
}




package com.looc.d03ip;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import scala.Tuple2;

import java.util.List;

/**
 * @author chenPeng
 * @version 1.0.0
 * @ClassName ReadRuler.java
 * @Description TODO将规则读取
 * @createTime 2019年05月11日 00:24:00
 */
public class MyUtil {
    /**
     * @return java.util.List<scala.Tuple2 < scala.Tuple2 < java.lang.Integer, java.lang.Integer>,java.lang.String>>
     * @Author chenpeng
     * @Description //TODO 通过spark来读取数据
     * @Date 2:17
     * @Param [rulerPath]
     **/
    public static List<Tuple2<Tuple2<Long, Long>, String>> readRuler(String rulerPath, JavaSparkContext jsc) {

        //读取文件
        JavaRDD<String> rulerText = jsc.textFile(rulerPath);

        //获取数据中的 ip十进制范围 地区名
        JavaPairRDD<Tuple2<Long, Long>, String> rule = rulerText.mapToPair(line -> {
            String[] lineArr = line.split("[|]");
            Long ipStart = Long.parseLong(lineArr[2]);
            Long ipEnd = Long.parseLong(lineArr[3]);

            Tuple2<Long, Long> ip = new Tuple2<>(ipStart, ipEnd);

            return new Tuple2<>(ip, lineArr[6] + "_" + lineArr[7]);
        });

        List<Tuple2<Tuple2<Long, Long>, String>> ruler = rule.collect();

        return ruler;
    }

    /**
     * @Author chenpeng
     * @Description //TODO 二分查找地点
     * @Date 2:34
     * @Param [rulers, ip]
     * @return java.lang.String
     **/
    public static String twoPoints(List<Tuple2<Tuple2<Long, Long>, String>> rulers, Long ip) {
        int begin = 0;
        int end = rulers.size() - 1;

        while (begin <= end) {
            int mid = (begin + end) / 2;
            Tuple2<Tuple2<Long, Long>, String> tMid = rulers.get(mid);
            if (tMid._1._1 <= ip && tMid._1._2 >= ip) {
                return tMid._2;
            } else if (ip > tMid._1._2) {
                begin = mid + 1;
            } else if (ip < tMid._1._1) {
                end = mid - 1;
            }
        }

        return "未知地点";
    }
}


```