# 1. Hive On Spark
> 与Hive没有太大的关系，使用了Hive的标准        

1. Hive的计算模型是MR，速度慢，而且还将中间数据写入HDFS中
2. Hive On Spark使用RDD（DataFrame），然后提交到spark集群上
3. hive跟mysql的区别
    1. hive是一个数据仓库（存储数据并分析数据，分析数据仓库中的数据量很大，一般要分析很长的时间）    
    2. mysql是一个关系型数据库（关系型数据的增删改查（低延迟））
4. Hive的元数据库中不保存数据，保存Hive仓库的表，字段，描述信息等
5. 实际数据保存在HDFS
6. Hive元数据库的功能：建立映射关系，执行HQL时，会想到MySQL元数据库中查找描述信息，然后根据描述信息生成任务，然后下发到spark集群中执行 

# 2. 配置   
1. 在Spark的conf目录下面创建一个hive的配置文件  
```
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://node:3306/hivedb?createDatabaseIfNotExist=true</value>
    <description>JDBC connect string for a JDBC metastore</description>
  </property>

   <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.jdbc.Driver</value>
    <description>Driver class name for a JDBC metastore</description>
  </property>

  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>bigdata</value>
    <description>username to use against metastore database</description>
  </property>

  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>123568</value>
    <description>password to use against metastore database</description>
  </property>

</configuration>
```

2. 上传mysql驱动jar包
    - 方案一：在Spark目录的jars下面
    - 方案二：放在自定义目录 输入目录`./spark-sql --master spark://node-4:7077 --driver-class-path /home/mysql-connector-java-5.1.7-bin.jar
`
    

3. 如果是新配置的Hive，否则跳过此步骤   
   -  修改配置中的MySQL数据库中修改DBS表中的DB_LOCATION_UIR改成hdfs的地址


4. 在环境中配置HDFS的位置
    `/etc/profile`中配置一个环节变量(让sparkSQL知道hdfs(namenode)在哪里)
    ```
    exprot HADOOP_CONF_DIR=...../hadoop/etc/hadoop
    ```
    
5. 重启SparkSQL的命令行


# 3. 使用
## 3.1. 命令行
```
~/apps/bigdata/spark/spark-2.4.2-bin-hadoop2.7/bin/spark-sql --master spark://vm04:7077
```

## 3.2. sql文件
```
/bigdata/spark-2.2.0-bin-hadoop2.7/bin/spark-sql --master spark://node:7077 --driver-class-path /home/xiaoniu/mysql-connector-java-5.1.7-bin.jar -f hive-sqls.sql
```
```
// -e 后面可以跟命令
-e "show database"
```


## 3.3. 在idea中开发
1. pom.xml
```
<!-- https://mvnrepository.com/artifact/org.apache.spark/spark-hive -->
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-hive_2.12</artifactId>
    <version>2.4.3</version>
    <scope>provided</scope>
</dependency>

```
2. java
```
SparkSession.builder()
      .appName("HiveOnSpark")
      .master("local[*]")
      .enableHiveSupport()//启用spark对hive的支持(可以兼容hive的语法了)
      .getOrCreate()
```

3. 指定hive元数据库的位置，添加hive-site.xml