[TOC]

# 1. SparkSQL简介
- SparkSQL是Spark上的一个SQL解析引擎，将SQL解析成特殊的RDD（DataFrame），然后在Spark集群中运行  
- SparkSQL是用来处理结构化数据的（现将非结构化的数据转换成结构化数据）
- SparkSQL支持两种编程API
    - SQL方式
    - DataFrame的方式
- SparkSQL兼容hive （元数据库，SQL语法，UDF，序列化，反序列化机制）
- SparkSQL支持统一的数据源，可读取多种类型的数据
- SparkSQL提供了标准的连接（JDBC，ODBC），以后可以对接BI工具


# 2. DataFrames 
## 2.1 简介
- 与RDD类似，DataFrame也是一个分布式数据容器。但是DataFrame更想传统数据库的二维表格，除了记录数据还记录结构信息
![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190513011753.png)

## 2.2 RDD与DataFrame的区别
- DataFrame里面存放的数据结构化数据的描述信息
- DataFrame要有表头 描述信息（列数，列名，类型，是否为空）  
- DataFrame是特殊的RDD（RDD+Schema信息就可以变为DataFrame）


# 3. SparkSQL demo
## 3.1 配置pom.xml  
```
<!-- 导入sparkSql的依赖 -->
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-sql_2.12</artifactId>
    <version>${spark.version}</version>
</dependency>
```
## 3.2 创建方式
### 3.2.1 SparkSQL1.x
1. scala
```
 //提交的这个程序可以连接到Spark集群中
    val conf = new SparkConf().setAppName("SQLDemo1").setMaster("local[2]")

    //创建SparkSQL的连接（程序执行的入口）
    val sc = new SparkContext(conf)
    //sparkContext不能创建特殊的RDD（DataFrame）
    //将SparkContext包装进而增强
    val sqlContext = new SQLContext(sc)
    //创建特殊的RDD（DataFrame），就是有schema信息的RDD

    //先有一个普通的RDD，然后在关联上schema，进而转成DataFrame

    val lines = sc.textFile("hdfs://node-4:9000/person")
    //将数据进行整理
    val boyRDD: RDD[Boy] = lines.map(line => {
      val fields = line.split(",")
      val id = fields(0).toLong
      val name = fields(1)
      val age = fields(2).toInt
      val fv = fields(3).toDouble
      Boy(id, name, age, fv)
    })
    //该RDD装的是Boy类型的数据，有了shcma信息，但是还是一个RDD
    //将RDD转换成DataFrame
    //导入隐式转换
    import sqlContext.implicits._
    val bdf: DataFrame = boyRDD.toDF

    //变成DF后就可以使用两种API进行编程了
    //把DataFrame先注册临时表
    bdf.registerTempTable("t_boy")

    //书写SQL（SQL方法应其实是Transformation）
    val result: DataFrame = sqlContext.sql("SELECT * FROM t_boy ORDER BY fv desc, age asc")

    //查看结果（触发Action）
    result.show()

    sc.stop()
```
2. 
```
 val rowRDD: RDD[Row] = lines.map(line => {
      val fields = line.split(",")
      val id = fields(0).toLong
      val name = fields(1)
      val age = fields(2).toInt
      val fv = fields(3).toDouble
      Row(id, name, age, fv)
    })

    //结果类型，其实就是表头，用于描述DataFrame
    val sch: StructType = StructType(List(
      StructField("id", LongType, true),
      StructField("name", StringType, true),
      StructField("age", IntegerType, true),
      StructField("fv", DoubleType, true)
    ))

    //将RowRDD关联schema
    val bdf: DataFrame = sqlContext.createDataFrame(rowRDD, sch)


    //变成DF后就可以使用两种API进行编程了
    //把DataFrame先注册临时表
    bdf.registerTempTable("t_boy")

    //书写SQL（SQL方法应其实是Transformation）
    val result: DataFrame = sqlContext.sql("SELECT * FROM t_boy ORDER BY fv desc, age asc")

```

3. 
```
//将数据进行整理
    val rowRDD: RDD[Row] = lines.map(line => {
      val fields = line.split(",")
      val id = fields(0).toLong
      val name = fields(1)
      val age = fields(2).toInt
      val fv = fields(3).toDouble
      Row(id, name, age, fv)
    })

    //结果类型，其实就是表头，用于描述DataFrame
    val sch: StructType = StructType(List(
      StructField("id", LongType, true),
      StructField("name", StringType, true),
      StructField("age", IntegerType, true),
      StructField("fv", DoubleType, true)
    ))

    //将RowRDD关联schema
    val bdf: DataFrame = sqlContext.createDataFrame(rowRDD, sch)

    //不使用SQL的方式，就不用注册临时表了
    val df1: DataFrame = bdf.select("name", "age", "fv")

    import sqlContext.implicits._

    val df2: DataFrame = df1.orderBy($"fv" desc, $"age" asc)


    df2.show()
```

### 3.2.2 SparkSQL2.x
1. 第一种方式
```java
public class SQLDemo2 {
    public static void main(String[] args) {
        //Spark2.x SQL执行入口
        SparkSession session = SparkSession.builder()
                .appName("SQLTest1")
                .master("local[4]")
                .getOrCreate();
        //创建RDD
        JavaRDD<String> rdd = session.sparkContext().textFile("H:\\javaCode\\ideaCode\\Android\\SparkDemo\\file\\tea.txt", 2).toJavaRDD();

        //将数据整理
        JavaRDD<Row> rowJavaRDD = rdd.map(line -> {
            String[] split = line.split(",");

            Integer id = Integer.parseInt(split[0]);
            String name = split[1];
            Integer age = Integer.parseInt(split[2]);
            Double nub = Double.parseDouble(split[3]);
            return RowFactory.create(id, name, age, nub);
        });

        //定义schema，描述DataFrame(表头)
        List<StructField> fields = new ArrayList<>();
        fields.add(DataTypes.createStructField("id", DataTypes.IntegerType, true));
        fields.add(DataTypes.createStructField("name", DataTypes.StringType, true));
        fields.add(DataTypes.createStructField("age", DataTypes.IntegerType, true));
        fields.add(DataTypes.createStructField("nub", DataTypes.DoubleType, true));

        StructType structType = DataTypes.createStructType(fields);


        //创建DataFrame
        Dataset<Row> dataFrame = session.createDataFrame(rowJavaRDD, structType);
        //显示表
        dataFrame.show();

        //创建零时视图
        dataFrame.createOrReplaceTempView("boy");

        Dataset<Row> result = session.sql("SELECT * FROM boy ORDER BY nub desc,age");

        result.show();
        
       //写入数据库
        Properties properties = new Properties();
        properties.setProperty("user","root");
        properties.setProperty("password","root");
        result.write().jdbc("jdbc:mysql://192.169.1.170:3306/bigdata?charatorEncoding=utf-8", "boys",properties);

        
        // SaveMode.Append表示添加的模式

        // SaveMode.Append:在数据源后添加；
        // SaveMode.Overwrite:如果如果数据源已经存在记录，则覆盖；
        // SaveMode.ErrorIfExists:如果如果数据源已经存在记录，则包异常；
        // SaveMode.Ignore:如果如果数据源已经存在记录，则忽略；
        //result.write(SaveMode.Append).mode().jdbc(）

        session.stop();
    }

}

/**
 * +---+----+---+-----+
 * | id|name|age|  nub|
 * +---+----+---+-----+
 * |  1|李四| 25| 99.0|
 * |  2|张三| 28|100.0|
 * |  3|王五| 25| 98.0|
 * |  4|麻子| 45| 99.0|
 * +---+----+---+-----+
 * 
 * 
 * +---+----+---+-----+
 * | id|name|age|  nub|
 * +---+----+---+-----+
 * |  2|张三| 28|100.0|
 * |  1|李四| 25| 99.0|
 * |  4|麻子| 45| 99.0|
 * |  3|王五| 25| 98.0|
 * +---+----+---+-----+
 **/
```

2. 第二种方式
```
public class SQLDemo3 {
    public static void main(String[] args) {
        //Spark2.x SQL执行入口
        SparkSession session = SparkSession.builder()
                .appName("SQLTest1")
                .master("local[4]")
                .getOrCreate();
        //创建RDD
        JavaRDD<String> rdd = session.sparkContext().textFile("H:\\javaCode\\ideaCode\\Android\\SparkDemo\\file\\tea.txt", 2).toJavaRDD();

        //将数据整理
        JavaRDD<Boy> BoyRDD = rdd.map(line -> {
            String[] split = line.split(",");

            Integer id = Integer.parseInt(split[0]);
            String name = split[1];
            Integer age = Integer.parseInt(split[2]);
            Double nub = Double.parseDouble(split[3]);
            return new Boy(id, name, age, nub);
        });

        // 对JavaBeans的RDD指定schema得到DataFrame
        Dataset<Row> peopleDF = session.createDataFrame(BoyRDD, Boy.class);


        //创建零时视图
        peopleDF.createOrReplaceTempView("boy");

        Dataset<Row> result = session.sql("SELECT * FROM boy ORDER BY nub desc,age");

        result.show();

        //写入数据库
        Properties properties = new Properties();
        properties.setProperty("user","root");
        properties.setProperty("password","root");
        result.write().mode(SaveMode.Append).jdbc(
                "jdbc:mysql://192.169.1.170:3306/bigdata?charatorEncoding=utf-8", "boys",properties);


        session.stop();
    }

    public static class Boy implements Serializable {
        private Integer id;
        private String name;
        private Integer age;
        private Double nub;

        public Boy() {
        }

        public Boy(Integer id, String name, Integer age, Double nub) {
            this.id = id;
            this.name = name;
            this.age = age;
            this.nub = nub;
        }

        public Integer getId() {
            return id;
        }

        public void setId(Integer id) {
            this.id = id;
        }

        public String getName() {
            return name;
        }

        public void setName(String name) {
            this.name = name;
        }

        public Integer getAge() {
            return age;
        }

        public void setAge(Integer age) {
            this.age = age;
        }

        public Double getNub() {
            return nub;
        }

        public void setNub(Double nub) {
            this.nub = nub;
        }
    }
}
```

## 3.3 wordcount SparkDemo
```
public class SQLDemo4 {
    public static void main(String[] args) {
        SparkSession session = SparkSession.builder().
                appName("SparkSession").
                master("local[*]").
                getOrCreate();

        //读取文件
        JavaRDD<String> file = session.read().textFile(
                "H:\\javaCode\\ideaCode\\Android\\SparkDemo\\file\\word.txt").toJavaRDD();

        //切片压平
        JavaRDD<Word> words = file.flatMap(lines -> {
            String[] line = lines.split(" ");

            List<Word> w = new ArrayList<>();
            for (String s : line) {
                 w.add(new Word(s));
            }

            return w.iterator();
        });


        Dataset<Row> dataFrame = session.createDataFrame(words, Word.class);

        dataFrame.createOrReplaceTempView("wordCountView");
        dataFrame.show();

        Dataset<Row> result = session.sql(
                "select word,count(*) nub FROM wordCountView group by word order by nub desc,word");
        result.show();

        session.stop();
    }


    public static class Word{
        private String word;

        public Word() {
        }

        public Word(String word) {
            this.word = word;
        }

        public String getWord() {
            return word;
        }

        public void setWord(String word) {
            this.word = word;
        }

    }
}
```

## 3.4 join
```
package com.looc.d06_spark_sql;

import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

/**
 * @author chenPeng
 * @version 1.0.0
 * @ClassName JoinDemo1.java
 * @Description TODO
 * @createTime 2019年05月13日 20:06:00
 */
public class JoinDemo1 {
    public static void main(String[] args) {
        SparkSession session = SparkSession.builder()
                .appName("SparkSession")
                .master("local[*]")
                .getOrCreate();

        //建立数据1
        JavaRDD<User> userJavaRDD = session.read()
                .textFile("H:\\javaCode\\ideaCode\\Android\\SparkDemo\\file\\user.txt")
                .toJavaRDD()
                .map(f -> {
                    String[] split = f.split(",");
                    return new User(Integer.parseInt(split[0]), split[1], split[2]);
                });
        Dataset<Row> userDate = session.createDataFrame(userJavaRDD, User.class);


        //建立数据2
        JavaRDD<User2> countryJavaRDD = session.read()
                .textFile("H:\\javaCode\\ideaCode\\Android\\SparkDemo\\file\\user2.txt")
                .toJavaRDD()
                .map(f -> {
                    String[] split = f.split(",");
                    return new User2(Integer.parseInt(split[0]), split[1]);
                });
        Dataset<Row> coutryData = session.createDataFrame(countryJavaRDD, User2.class);


        userDate.createOrReplaceTempView("user");
        coutryData.createOrReplaceTempView("country");

        Dataset<Row> result = session.sql("SELECT * from user inner join country on user.country = country.id");
        result.show();

        session.stop();
    }

    public static class User2{
        private Integer id;
        private String name;

        public User2() {
        }

        public User2(Integer id, String name) {
            this.id = id;
            this.name = name;
        }

        public Integer getId() {
            return id;
        }

        public void setId(Integer id) {
            this.id = id;
        }

        public String getName() {
            return name;
        }

        public void setName(String name) {
            this.name = name;
        }
    }


    public static class User{
        private Integer id;
        private String name;
        private String country;

        public User(int i, String s) {
        }

        public User(Integer id, String name, String country) {
            this.id = id;
            this.name = name;
            this.country = country;
        }

        public Integer getId() {
            return id;
        }

        public void setId(Integer id) {
            this.id = id;
        }

        public String getName() {
            return name;
        }

        public void setName(String name) {
            this.name = name;
        }

        public String getCountry() {
            return country;
        }

        public void setCountry(String country) {
            this.country = country;
        }
    }
}

```

# 4. Dataset
```
Dateset是spark1.6以后推出的新的API，也是一个分布式数据集，于RDD相比，保存了跟多的描述信息，概念上等同于关系型数据库中的二维表，基于保存了跟多的描述信息，spark在运行时可以被优化。

Dateset里面对应的的数据是强类型的，并且可以使用功能更加丰富的lambda表达式，弥补了函数式编程的一些缺点，使用起来更方便

在scala中，DataFrame其实就是Dateset[Row]

Dataset的特点：
	1.一系列分区
	2.每个切片上会有对应的函数
	3.依赖关系
	4.kv类型shuffle也会有分区器
	5.如果读取hdfs中的数据会感知最优位置
	6.会优化执行计划
	7.支持更加智能的数据源


调用Dataset的方法先会生成逻辑计划，然后被spark的优化器进行优化，最终生成物理计划，然后提交到集群中运行！
```

# 5. SparkSQL的各种数据读取与写入
## 5.1 从JDBC读取
1. 建立连接
> 这个时候会建立连接，但是只会拿到列名，不会读取数据
```
Dataset<Row> df = spark.read().
                format("jdbc")
                .option("url", "jdbc:mysql://192.169.1.170:3306/bigdata")
                .option("dbtable", "emp")
                .option("user", "root")
                .option("password", "root")
                .load();
```
2. 支持filter 约定于 where
```
//方法一
Dataset<Row> filter = df.filter(new FilterFunction<Row>() {
    @Override
    public boolean call(Row value) throws Exception {
        return value.getInt(0)>=1203;
    }
});
//方法二
Dataset<Row> filter1 = df.filter("id<=1203");
filter1.show();
```

3. 支持select方法
```
Dataset<Row> select = df.selectExpr("id","name","salary/10 as sum");
select.show();
```

## 5.2 写入数据库
```
Properties properties = new Properties();
properties.setProperty("user", "root");
properties.setProperty("password", "root");

select.write().mode(SaveMode.Append).
        jdbc("jdbc:mysql://192.169.1.170:3306/bigdata","empTest",properties);
```

## 5.3 保存到text
> 只能保存一列数据
```
select.selectExpr("name").write().
        mode(SaveMode.Append).
        text("C:\\Users\\81022\\Desktop\\test\\text");
```
## 5.4 保存到json
> 会记录列名，在读取时会自动判断数据格式
```
select.write().
    mode(SaveMode.Append).
    json("C:\\Users\\81022\\Desktop\\test\\json");
```
## 5.5 保存到csv
> 不保存名称以及类型，实际上列通过‘,’来分隔
```
 select.write().
        mode(SaveMode.Append).
        csv("C:\\Users\\81022\\Desktop\\test\\csv");
```
## 5.6 保存到parquet
>压缩了 带有各个数据的类型与name
```
select.write().
    mode(SaveMode.Append).
    parquet("C:\\Users\\81022\\Desktop\\test\\parquet");
```
## 5.7 读取 
```
Dataset<Row> json = spark.read().json("C:\\Users\\81022\\Desktop\\test\\json");
json.show();

//全都按string
spark.read().csv("C:\\Users\\81022\\Desktop\\test\\csv").show();

//最常用
Dataset<Row> parquet = spark.read().parquet("C:\\Users\\81022\\Desktop\\test\\parquet");
parquet.printSchema();
parquet.show();
```


## code ：
```
public class SQLDataSource {
    public static void main(String[] args) {
        SparkSession spark = SparkSession.builder()
                .appName("Spark")
                .master("local[*]")
                .getOrCreate();

        Dataset<Row> df = spark.read().
                format("jdbc")
                .option("url", "jdbc:mysql://192.169.1.170:3306/bigdata")
                .option("dbtable", "emp")
                .option("user", "root")
                .option("password", "root")
                .load();

        df.printSchema();

        df.show();

        //过滤 方法1
        Dataset<Row> filter = df.filter(new FilterFunction<Row>() {
            @Override
            public boolean call(Row value) throws Exception {
                return value.getInt(0)>=1203;
            }
        });
        filter.show();
        //过滤 方法2
        Dataset<Row> filter1 = df.filter("id<=1203");
        filter1.show();

        //选择
        Dataset<Row> select = df.selectExpr("id","name","salary/10 as sum");
        select.show();


        //写入数据库
        Properties properties = new Properties();
        properties.setProperty("user", "root");
        properties.setProperty("password", "root");

        select.write().mode(SaveMode.Append).
                jdbc("jdbc:mysql://192.169.1.170:3306/bigdata","empTest",properties);

        //--保存-------------------------------
        //保存纯文本只能保存一列数据
        select.selectExpr("name").write().
                mode(SaveMode.Append).
                text("C:\\Users\\81022\\Desktop\\test\\text");
        //json会带数据名
        select.selectExpr("name").write().
                mode(SaveMode.Append).
                json("C:\\Users\\81022\\Desktop\\test\\json");
        //不会记录数列名
        select.selectExpr("name").write().
                mode(SaveMode.Append).
                csv("C:\\Users\\81022\\Desktop\\test\\csv");
        //压缩了
        select.selectExpr("name").write().
                mode(SaveMode.Append).
                parquet("C:\\Users\\81022\\Desktop\\test\\parquet");



        spark.stop();
    }
}
```


```
public class SQLDataSource2 {
    public static void main(String[] args) {
        SparkSession spark = SparkSession.builder()
                .appName("Spark")
                .master("local[4]")
                .getOrCreate();
        //-----读取------------------------
        //会自动判断时间类型 只有简单的几种
        Dataset<Row> json = spark.read().json("C:\\Users\\81022\\Desktop\\test\\json");
        json.show();

        //全都按string
        spark.read().csv("C:\\Users\\81022\\Desktop\\test\\csv").show();

        //最常用
        Dataset<Row> parquet = spark.read().parquet("C:\\Users\\81022\\Desktop\\test\\parquet");
        parquet.printSchema();
        parquet.show();



        //----其他操作--------------------------
        Dataset<String> stringDataset = json.flatMap((FlatMapFunction<Row, String>) name -> {
            return Arrays.asList(name.getString(0).split("")).iterator();
        }, Encoders.STRING());

        //wordCount
        Dataset<Row> word = stringDataset.toDF("word");
        word.show();
        word.createOrReplaceTempView("words");

        Dataset<Row> sql = spark.sql("select word,count(*) as nub from words group by word order by nub desc,word");
        sql.show();

        //随便写的
        Dataset<Tuple2<String, Integer>> map = stringDataset.map(new MapFunction<String, Tuple2<String, Integer>>() {
            @Override
            public Tuple2<String, Integer> call(String value) throws Exception {
                return new Tuple2<>(value, 1);
            }
        }, Encoders.tuple(Encoders.STRING(), Encoders.INT()));



        Tuple2<String, Integer> reduce = map.reduce(new ReduceFunction<Tuple2<String, Integer>>() {
            @Override
            public Tuple2<String, Integer> call(Tuple2<String, Integer> v1, Tuple2<String, Integer> v2) throws Exception {
                return new Tuple2<>(v1._1, v1._2 + v2._2);
            }
        });

        System.out.println(reduce._2+"      "+reduce._1);
        map.show();

        spark.stop();
    }
}
```

# 6 使用SparkSQL 来统计每门课中最受欢迎的前3位老师
```
package com.looc.d07sql;

import org.apache.spark.api.java.function.MapFunction;
import org.apache.spark.sql.*;
import scala.Tuple2;

/**
 * @author chenPeng
 * @version 1.0.0
 * @ClassName SQLFavTeache.java
 * @Description TODO 使用SparkSQL 来统计每门课中最受欢迎的前3位老师
 * @createTime 2019年05月15日 14:53:00
 */
public class SQLFavTeache {
    private static String textSrc = "H:\\javaCode\\ideaCode\\Android\\SparkDemo\\file\\teacher.log";
    public static void main(String[] args) {
        SparkSession spark = SparkSession.builder()
                .appName("spark")
                .master("local[2]")
                .getOrCreate();

        //读取并处理数据
        Dataset<Row> df = spark.read().textFile(textSrc).map(new MapFunction<String, Tuple2<String, String>>() {
            @Override
            public Tuple2<String, String> call(String value) throws Exception {
                String[] split = value.split("/");
                String subject = split[2].split("[.]")[0];

                return new Tuple2<>(subject, split[3]);
            }
        }, Encoders.tuple(Encoders.STRING(), Encoders.STRING())).toDF("subject", "teacher");

        df.createOrReplaceTempView("v_sub_teacher");

        //统计出某学科某老师的访问量
        Dataset<Row> temp1 = spark.sql("SELECT subject,teacher,count(*) counts FROM v_sub_teacher GROUP BY subject,teacher");
        temp1.createOrReplaceTempView("v_sub_teacher_count");

        //每个学科下面最受欢迎的老师
        Dataset<Row> temp2 = spark.sql("SELECT subject,teacher,counts,row_number() over(partition by subject order by counts desc) rk ,dense_rank() over(order by counts desc) ark FROM v_sub_teacher_count");
        temp2.createOrReplaceTempView("v_sub_teacher_count_top");

        //拿出前两名 并在里面进行全局排序
        Dataset<Row> result = spark.sql("SELECT subject,teacher,counts,rk,ark FROM v_sub_teacher_count_top WHERE rk<=2");

        result.show();


        spark.stop();
    }
}

```