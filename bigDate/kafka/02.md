[TOC]
# 1. 简介
![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190521164019.png)

# 2. 测试
1. maven 中配置依赖
```
        <!-- 导入sparkSql的依赖 -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.12</artifactId>
            <version>${spark.version}</version>
        </dependency>
```
2. java代码
```
public class SparkStreamingDemo1 {
    public static void main(String[] args) throws InterruptedException {
        SparkConf sparkConf =
                new SparkConf().setAppName("StreamingWordCont").setMaster("local[2]");
        // StreamingContext 是对SparkContext的包装，包了一层增加实时的功能
        // 第二个参数是小批次产生的时间间隔
        JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, Durations.seconds(10));

        //创建一个DStream来表示来自TCP源的流数据，指定为主机名（例如localhost）和端口（例如9999）
        JavaReceiverInputDStream<String> line = jssc.socketTextStream("192.169.1.11", 9999);

        //切分压平
        JavaDStream<String> words = line.flatMap(f -> Arrays.asList(f.split(" ")).iterator());

        //创建
        JavaPairDStream<String, Integer> word = words.mapToPair(f -> new Tuple2<>(f, 1));
        //聚合
        JavaPairDStream<String, Integer> wordCounts = word.reduceByKey((a, b) -> a + b);

        //打印输出
        wordCounts.print();

        //启动SparkSteaming
        jssc.start();
        //等待退出
        jssc.awaitTermination();


    }
}
```

3. 在linux服务器上启动 nc命令
![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190521193506.png)

# 3. 与Kafka整合 0.8(未实现)
1. pom.xml
```
        <!--导入streaming 与 kafka的依赖-->
        <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-kafka-0-8 -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming-kafka-0-8_2.11</artifactId>
            <version>2.4.3</version>
        </dependency>
```

2. 

3. 启动卡夫卡生成者
```
# 创建一个topic
/home/chenpeng/apps/bigdate/kafka/kafka_2.11-0.8.2.2/bin/kafka-topics.sh --create --zookeeper vm01:2181,vm02:2181,vm03:2181 --replication-factor 3 --partitions 3 --topic testWordcount

# 开启这个topid的生产者
/home/chenpeng/apps/bigdate/kafka/kafka_2.11-0.8.2.2/bin/kafka-console-producer.sh --broker-list vm01:9092,vm02:9092,vm03:9092 --topic testWordcount
```







1.Spark Streaming是一个基于Spark Core之上的实时计算框架，可以从很多数据源消费数据并对数据进行处理，
在Spark Streaing中有一个最基本的抽象叫DStream（代理），本质上就是一系列连续的RDD，DStream其实就是对RDD的封装
DStream可以任务是一个RDD的工厂，该DStream里面生产都是相同业务逻辑的RDD，只不过是RDD里面要读取数据的不相同

深入理解DStream:他是sparkStreaming中的一个最基本的抽象，代表了一下列连续的数据流，本质上是一系列连续的RDD，你对DStream进行操作，就是对RDD进行操作

DStream每隔一段时间生成一个RDD，你对DStream进行操作，本质上是对里面的对应时间的RDD进行操作

DSteam和DStream之间存在依赖关系，在一个固定的时间点，对个存在依赖关系的DSrteam对应的RDD也存在依赖关系，
每个一个固定的时间，其实生产了一个小的DAG，周期性的将生成的小DAG提交到集群中运行

# 4. kafka1.0以上
1. pom.xml
```xml
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-streaming-kafka-0-10_2.12</artifactId>
    <version>2.4.3</version>
</dependency>
<!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients -->
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-clients</artifactId>
    <version>2.0.0</version>
</dependency>
<!-- https://mvnrepository.com/artifact/org.apache.spark/spark-tags -->
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-tags_2.12</artifactId>
    <version>2.4.3</version>
</dependency>
```

2. java
```java
package com.looc.d09spark_streaming;

import org.apache.spark.SparkConf;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.*;
import org.apache.spark.streaming.kafka010.*;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.common.serialization.StringDeserializer;
import scala.Tuple2;

import java.util.Arrays;
import java.util.Collection;
import java.util.HashMap;
import java.util.Map;

/**
 * @author chenPeng
 * @version 1.0.0
 * @ClassName KafkaSparkTest.java
 * @Description TODO
 * @createTime 2019年05月22日 16:35:00
 */
public class KafkaSparkTest {
    public static void main(String[] args) throws InterruptedException {
        //创建SparkConf
        SparkConf conf = new SparkConf().setAppName("KafkaDirectWordCount").setMaster("local[2]");

        //创建SparkStreaming，并设置间隔时间
        JavaStreamingContext jssc = new JavaStreamingContext(conf, new Duration(5000));

        Map<String, Object> kafkaParams = new HashMap<>(16);
        kafkaParams.put("bootstrap.servers", "vm01:9092,vm02:9092,vm03:9092");
        kafkaParams.put("key.deserializer", StringDeserializer.class);
        kafkaParams.put("value.deserializer", StringDeserializer.class);
        kafkaParams.put("group.id", "use_a_separate_group_id_for_each_stream");
        //auto.offset.reset值含义解释
        // earliest
        //      当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费
        // latest
        //      当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据
        // none
        //      topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常
        // smallest
        //      从头开始消费
        kafkaParams.put("auto.offset.reset", "earliest");
        kafkaParams.put("enable.auto.commit", false);

        //设置topic
        Collection<String> topics = Arrays.asList("chen");

        JavaInputDStream<ConsumerRecord<String, String>> stream = KafkaUtils.createDirectStream(jssc,
                LocationStrategies.PreferBrokers()
                , ConsumerStrategies.<String, String>Subscribe(topics, kafkaParams));

        //切分压平
        JavaDStream<String> words = stream.flatMap(f -> Arrays.asList(f.value().split(" ")).iterator());

        //单词形成 (word，1)的形式
        JavaPairDStream<String, Integer> word = words.mapToPair(f -> new Tuple2<>(f,1));

        //分组聚合
        JavaPairDStream<String, Integer> result = word.reduceByKey((a, b) -> a + b);

        result.print();

        jssc.start();

        jssc.awaitTermination();

    }
}

```

https://blog.csdn.net/daerzei/article/details/80085121

https://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html#spark-streaming-kafka-integration-guide-kafka-broker-version-010
## 4.1 记录偏移量到kafaka
```
public class KafkaSparkZk {
    public static void main(String[] args) throws InterruptedException {
        //创建SparkConf
        SparkConf conf = new SparkConf().setAppName("KafkaDirectWordCount").setMaster("local[2]");

        //创建SparkStreaming，并设置间隔时间
        JavaStreamingContext jssc = new JavaStreamingContext(conf, new Duration(5000));

        Map<String, Object> kafkaParams = new HashMap<>(16);
        kafkaParams.put("bootstrap.servers", "vm01:9092,vm02:9092,vm03:9092");
        kafkaParams.put("key.deserializer", StringDeserializer.class);
        kafkaParams.put("value.deserializer", StringDeserializer.class);
        kafkaParams.put("group.id", "use_a_separate_group_id_for_each_stream");
        kafkaParams.put("auto.offset.reset", "earliest");
        kafkaParams.put("enable.auto.commit", false);

        //设置topic
        Collection<String> topics = Arrays.asList("chen");

        JavaInputDStream<ConsumerRecord<String, String>> stream = KafkaUtils.createDirectStream(jssc,
                LocationStrategies.PreferBrokers()
                , ConsumerStrategies.<String, String>Subscribe(topics, kafkaParams));

        //记录偏移量和记录
        stream.foreachRDD(rdd->{
            OffsetRange[] offsetRanges = ((HasOffsetRanges) rdd.rdd()).offsetRanges();
            rdd.foreach(f->System.out.println(" ====="+f.value()));

            // some time later, after outputs have completed
            ((CanCommitOffsets) stream.inputDStream()).commitAsync(offsetRanges);
        });


        jssc.start();

        jssc.awaitTermination();

    }
}
```

## 4.2 存储到自己的存储
```
// The details depend on your data store, but the general idea looks like this

// begin from the the offsets committed to the database
Map<TopicPartition, Long> fromOffsets = new HashMap<>();
for (resultSet : selectOffsetsFromYourDatabase)
  fromOffsets.put(new TopicPartition(resultSet.string("topic"), resultSet.int("partition")), resultSet.long("offset"));
}

JavaInputDStream<ConsumerRecord<String, String>> stream = KafkaUtils.createDirectStream(
  streamingContext,
  LocationStrategies.PreferConsistent(),
  ConsumerStrategies.<String, String>Assign(fromOffsets.keySet(), kafkaParams, fromOffsets)
);

stream.foreachRDD(rdd -> {
  OffsetRange[] offsetRanges = ((HasOffsetRanges) rdd.rdd()).offsetRanges();
  
  Object results = yourCalculation(rdd);

  // begin your transaction

  // update results
  // update offsets where the end of existing offsets matches the beginning of this batch of offsets
  // assert that offsets were updated correctly

  // end your transaction
});
```

# DStream
![](https://raw.githubusercontent.com/shutter-cp/imgBed/master/img/20190521221452.png)
