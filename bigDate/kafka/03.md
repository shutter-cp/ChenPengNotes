[TOC]
# 1. 主程序
> 利用sparkStreaming 从kafka中读取数据  
> 其中的根据地区来统计销量 ip匹配规则要通过广播发送
```
package com.looc.d10sparkstreaming_kafka_redis;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.broadcast.Broadcast;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka010.*;
import scala.Tuple2;

import java.util.*;

/**
 * @author chenPeng
 * @version 1.0.0
 * @ClassName OrderCount.java
 * @Description TODO 主入口
 * @createTime 2019年05月23日 01:58:00
 */
public class OrderCount {
    public static void main(String[] args) throws InterruptedException{
        //创建SparkConf
        SparkConf conf = new SparkConf().setAppName("KafkaDirectWordCount").setMaster("local[2]");

        //创建SparkStreaming，并设置间隔时间
        JavaStreamingContext jssc = new JavaStreamingContext(conf, new Duration(5000));

        Map<String, Object> kafkaParams = new HashMap<>(16);
        kafkaParams.put("bootstrap.servers", "vm01:9092,vm02:9092,vm03:9092");
        kafkaParams.put("key.deserializer", StringDeserializer.class);
        kafkaParams.put("value.deserializer", StringDeserializer.class);
        kafkaParams.put("group.id", "use_a_separate_group_ir_each_stream");
        kafkaParams.put("auto.offset.reset", "earliest");
        kafkaParams.put("enable.auto.commit", false);

        //设置topic
        Collection<String> topics = Arrays.asList("orderss");


        //拿到规则的广播
        Broadcast<List<Tuple2<Tuple2<Long, Long>, String>>> listBroadcast =
                IPUtils.readRuler("H:\\javaCode\\ideaCode\\Android\\SparkDemo\\file\\ip.txt",jssc);


        JavaInputDStream<ConsumerRecord<String, String>> stream = KafkaUtils.createDirectStream(jssc,
                LocationStrategies.PreferBrokers()
                , ConsumerStrategies.<String, String>Subscribe(topics, kafkaParams));

        stream.foreachRDD(rdd->{
            OffsetRange[] offsetRanges = ((HasOffsetRanges) rdd.rdd()).offsetRanges();
            
            
            //实现逻辑
            if (!rdd.isEmpty()){
                //将数据切分
                JavaRDD<String[]> map = rdd.map(f -> f.value().split(" "));
                map.foreach(f->System.out.println(Arrays.asList(f)));

                //计算成交总金额
                CalculateUtil.totalAmount(map);

                //计算商品分类金额
                CalculateUtil.classificationAmount(map);

                //计算区域成交金额
                CalculateUtil.regionalAmount(map,listBroadcast);
                    // some time later, after outputs have completed
            }

            ((CanCommitOffsets) stream.inputDStream()).commitAsync(offsetRanges);
        });



        jssc.start();

        jssc.awaitTermination();
    }
}

```

# 2. 运算实现
```
package com.looc.d10sparkstreaming_kafka_redis;

import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.broadcast.Broadcast;
import redis.clients.jedis.Jedis;
import scala.Tuple2;

import java.util.List;

/**
 * @author chenPeng
 * @version 1.0.0
 * @ClassName CalculateUtil.java
 * @Description TODO
 * @createTime 2019年05月23日 02:01:00
 */
public class CalculateUtil {
    /**
     * @Author chenpeng
     * @Description //TODO 计算总金额
     * @Date 2:23 
     * @Param [map]
     * @return void
     **/
    public static void totalAmount(JavaRDD<String[]> map) {
        //将价格取出
        JavaRDD<Double> rdd = map.map(f -> Double.parseDouble(f[4]));
        //聚合
        Double reduce = rdd.reduce((a, b) -> a + b);

        //将数据写人redis
        Jedis connection = JedisConnectionPool.getConnection();
        connection.incrByFloat("ALLAmount", reduce);
        connection.close();
    }
    
    
    /**
     * @Author chenpeng
     * @Description //TODO 计算分类金额
     * @Date 2:23 
     * @Param [map]
     * @return void
     **/
    public static void classificationAmount(JavaRDD<String[]> map) {
        JavaPairRDD<String, Double> rdd = map.mapToPair(f -> new Tuple2<>(f[2], Double.parseDouble(f[4])));
        JavaPairRDD<String, Double> reduce = rdd.reduceByKey((a, b) -> a + b);


        reduce.foreachPartition(f->{
            //将数据写人redis
            Jedis connection = JedisConnectionPool.getConnection();

            while (f.hasNext()){
                Tuple2<String, Double> re = f.next();
                connection.incrByFloat(re._1,re._2);
            }

            connection.close();
        });

    }

    /**
     * @Author chenpeng
     * @Description //TODO 计算地区金额
     * @Date 2:24 
     * @Param [map]
     * @return void
     **/
    public static void regionalAmount(JavaRDD<String[]> map, Broadcast<List<Tuple2<Tuple2<Long, Long>, String>>> listBroadcast) {
        JavaPairRDD<String, Double> rdd = map.mapToPair(f -> new Tuple2<>(f[1] , Double.parseDouble(f[4])));
        JavaPairRDD<String, Double> reduce = rdd.reduceByKey((a, b) -> a + b);


        JavaRDD<Tuple2<String,Double>> result = reduce.map(f -> {
            List<Tuple2<Tuple2<Long, Long>, String>> value = listBroadcast.value();
            String area = IPUtils.twoPoints(value, IPUtils.ipConversion(f._1));

            System.out.println(IPUtils.ipConversion(f._1));

            return new Tuple2<String,Double>(area, f._2);
        });

        result.foreachPartition(f->{
            //将数据写人redis
            Jedis connection = JedisConnectionPool.getConnection();
            while (f.hasNext()){
                Tuple2<String, Double> next = f.next();
                connection.incrByFloat(next._1, next._2);
            }
            connection.close();
        });

    }
}

```

# 3. ip规则工具类
```
package com.looc.d10sparkstreaming_kafka_redis;

import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.broadcast.Broadcast;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import scala.Tuple2;

import java.util.List;

/**
 * @author chenPeng
 * @version 1.0.0
 * @ClassName IPUtils.java
 * @Description TODO
 * @createTime 2019年05月23日 02:00:00
 */
public class IPUtils {
    /**
     * @Author chenpeng
     * @Description //TODO 读取规则
     * @Date 2:45 
     * @Param [rulerPath, jsc]
     * @return java.util.List<scala.Tuple2<scala.Tuple2<java.lang.Long,java.lang.Long>,java.lang.String>>
     **/
    public static Broadcast<List<Tuple2<Tuple2<Long, Long>, String>>> readRuler(String rulerPath, JavaStreamingContext jsc) {
        JavaSparkContext javaSparkContext = jsc.sparkContext();
        //读取文件
        JavaRDD<String> rulerText = javaSparkContext.textFile(rulerPath);

        //获取数据中的 ip十进制范围 地区名
        JavaPairRDD<Tuple2<Long, Long>, String> rule = rulerText.mapToPair(line -> {
            String[] lineArr = line.split("[|]");
            Long ipStart = Long.parseLong(lineArr[2]);
            Long ipEnd = Long.parseLong(lineArr[3]);

            Tuple2<Long, Long> ip = new Tuple2<>(ipStart, ipEnd);

            return new Tuple2<>(ip, lineArr[6] + "_" + lineArr[7]);
        });

        List<Tuple2<Tuple2<Long, Long>, String>> ruler = rule.collect();

        return javaSparkContext.broadcast(ruler);
    }

    /**
     * @Author chenpeng
     * @Description //TODO 点分十进制转十进制
     * @Date 2:45 
     * @Param [ip]
     * @return java.lang.Long
     **/
    public static Long ipConversion(String ip){
        String[] ipArr = ip.split("[.]");
        /*Long sc = 0L;

        for (int i = 0; i < ipArr.length; i++) {
            int temp = Integer.parseInt(ipArr[i]) << 8*(ipArr.length-i-1);
            sc = sc | temp;
        }*/

        long sc = 0L;
        for (int i = 0; i <= 3; i++) {
            long ips = Long.parseLong(ipArr[i]);
            sc |= ips << ((3-i) << 3);
        }
        return sc;
    }

    /**
     * @Author chenpeng
     * @Description //TODO 二分查找地点
     * @Date 2:34
     * @Param [rulers, ip]
     * @return java.lang.String
     **/
    public static String twoPoints(List<Tuple2<Tuple2<Long, Long>, String>> rulers, Long ip) {
        int begin = 0;
        int end = rulers.size() - 1;

        while (begin <= end) {
            int mid = (begin + end) / 2;
            Tuple2<Tuple2<Long, Long>, String> tMid = rulers.get(mid);
            if (tMid._1._1 <= ip && tMid._1._2 >= ip) {
                return tMid._2;
            } else if (ip > tMid._1._2) {
                begin = mid + 1;
            } else if (ip < tMid._1._1) {
                end = mid - 1;
            }
        }

        return "未知地点";
    }
}

```


# 4. redis 连接池
```java
package com.looc.d10sparkstreaming_kafka_redis;

import redis.clients.jedis.Jedis;
import redis.clients.jedis.JedisPool;
import redis.clients.jedis.JedisPoolConfig;

/**
 * @author chenPeng
 * @version 1.0.0
 * @ClassName JedisConnectionPool.java
 * @Description TODO
 * @createTime 2019年05月23日 01:58:00
 */
public class JedisConnectionPool {
    private static JedisPoolConfig config;
    private static JedisPool pool;
    static {
        config = new JedisPoolConfig();

        //设置最大连接数
        config.setMaxTotal(20);
        //设置最大空闲连接
        config.setMaxIdle(10);

        pool = new JedisPool(config, "192.169.1.11",6379, 10000, "root");
    }

    /**
     * @Author chenpeng
     * @Description //TODO 获取一个redis连接
     * @Date 2:14
     * @Param []
     * @return redis.clients.jedis.Jedis
     **/
    public static Jedis getConnection(){
         return pool.getResource();
    }
}

```


# 数据类型
```
id ip 类别 商品名 价格
```

# maven
```xml
        <!-- 导入spark的依赖 -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.12</artifactId>
            <version>${spark.version}</version>
        </dependency>
        
        <!--导入spark streaming的依赖-->
        <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-hive -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_2.12</artifactId>
            <version>2.4.3</version>
        </dependency>
        <!--导入streaming 与 kafka的依赖-->
        <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-kafka-0-10 -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming-kafka-0-10_2.12</artifactId>
            <version>2.4.3</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients -->
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <version>2.0.0</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-tags -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-tags_2.12</artifactId>
            <version>2.4.3</version>
        </dependency>

        <!--导入redis的依赖-->
        <!-- https://mvnrepository.com/artifact/redis.clients/jedis -->
        <dependency>
            <groupId>redis.clients</groupId>
            <artifactId>jedis</artifactId>
            <version>3.0.1</version>
        </dependency>
        <!-- 指定hadoop-client API的版本 -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>${hadoop.version}</version>
        </dependency>
        
```